<!DOCTYPE html>
<html>

<head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Backpropagation</title>
    <link rel="stylesheet" href="../style.css">
    <!--katex-->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.10.0-rc.1/dist/katex.min.css"
        integrity="sha384-D+9gmBxUQogRLqvARvNLmA9hS2x//eK1FhVb9PiU86gmcrBrJAQT8okdJ4LMp2uv" crossorigin="anonymous">
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.10.0-rc.1/dist/katex.min.js"
        integrity="sha384-483A6DwYfKeDa0Q52fJmxFXkcPCFfnXMoXblOkJ4JcA8zATN6Tm78UNL72AKk+0O"
        crossorigin="anonymous"></script>
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.10.0-rc.1/dist/contrib/auto-render.min.js"
        integrity="sha384-yACMu8JWxKzSp/C1YV86pzGiQ/l1YUfE8oPuahJQxzehAjEt2GiQuy/BIvl9KyeF" crossorigin="anonymous"
        onload="renderMathInElement(document.body);"></script>
    <!--highlight code-->
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.25.0/themes/prism.min.css"
        integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ=="
        crossorigin="anonymous" referrerpolicy="no-referrer" />
    <script defer src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.25.0/prism.min.js"
        integrity="sha512-hpZ5pDCF2bRCweL5WoA0/N1elet1KYL5mx3LP555Eg/0ZguaHawxNvEjF6O3rufAChs16HVNhEc6blF/rZoowQ=="
        crossorigin="anonymous" referrerpolicy="no-referrer"></script>
    <script defer src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.25.0/plugins/autoloader/prism-autoloader.min.js"
        integrity="sha512-sv0slik/5O0JIPdLBCR2A3XDg/1U3WuDEheZfI/DI5n8Yqc3h5kjrnr46FGBNiUAJF7rE4LHKwQ/SoSLRKAxEA=="
        crossorigin="anonymous" referrerpolicy="no-referrer"></script>
</head>

<body class="margin-0">
    <div class="page">
        <div class="flex align-center justify-center border-top border-bottom">
            <h1>Backpropagation</h1>
        </div>
        <div class="flex align-center">
            <a class="padding border-bottom" href="../index.html">back</a>
        </div>
        <h2>Regression</h2>
        <br>
        <p>
            Regression is the method by which you find the relationship between one or more independent variable(s)
            \(\small{x}\) and a dependent variable \(\small{y}\). A regression model assumes the following relationship
            holds among the dependent and independent variables
        </p>
        <p class=”text-center”>
            \(\small{y_i=f(x_i, \theta)+\epsilon}\)
        </p>
        <p>
            where \(\small{\theta}\) is the set of parameters we tune to fit the model, and \(\small{\epsilon}\) is the
            error term (no model is perfect!). Let’s take linear regression for example. It’s the simplest example of
            regression. You have a bunch of points \(\small{(x_i, y_i)}\), where \(\small{y_i}\) is the dependent
            variable, \(\small{x_i}\) is the independent variable, and we need to find a function
        </p>
        <p class=”text-center”>
            \(\small{f(x_i, \theta)=\theta_1+\theta_2x_i}\)
        </p>
        <p>
            so \(\small{f(x_i, \theta)}\) is as close as possible to \(\small{y_i}\). More precisely, we need to find
            \(\small{\theta_1}\) (y-intercept) and \(\small{\theta_2}\) (the slope) that best relates all \(\small{(x_i,
            y_i)}\) (where \(\small{\theta=\{\theta_1, \theta_2\}}\) is the set of parameters to tune). How do we find
            \(\small{\theta_1}\) and \(\small{\theta_2}\)? We need a way to measure how wrong the model is.
        </p>
        <p>
            Initially, we choose random values for \(\small{\theta_1}\) and \(\small{\theta_2}\). We tune
            \(\small{\theta_1}\) and \(\small{\theta_2}\) based on how wrong the model is until we reach
            \(\small{\theta^*}\), the optimal value. We measure the model’s wrongness using the mean-square error (MSE)
            loss function. For a data point \(\small{(x_i, y_i)}\), the actual value is \(\small{y_i}\), and the model
            prediction is \(\small{f(x_i, \theta)}\), and the MSE for this data point is
        </p>
        <p class=”text-center”>
            \(\small{\delta_i=(y_i-f(x_i, \theta))^2=(y_i-(\theta_1+\theta_2x_i))^2}\)
        </p>
        <p>
            and the total loss is the sum over \(\small{i}\)
        </p>
        <p>
            \(\small{L(\theta=\{\theta_1, \theta_2\})=\sum\limits_{i=1}^m\delta_i}\) \(\small{=\sum\limits_{i=1}^m(y_i-f(x_i,
            \theta))^2=\sum\limits_{i=1}^m(y_i-(\theta_1+\theta_2x_i))^2}\)
        </p>
        <p>
            Note that \(\small{L(\theta)}\) is a function of \(\small{\theta}\) (here, function of \(\small{\theta_1}\)
            and \(\small{\theta_2}\)), and \(\small{x_i}\) and \(\small{y_i}\) are treated as constants whereas
            \(\small{\theta_1}\) and \(\small{\theta_2}\) are variables. If actual value (or label) \(\small{y_i}\) is
            very different from prediction \(\small{f(x_i, \theta)}\), \(\small{\delta_i}\) is very large. The residual
            (difference b/w label and prediction) because we want to penalise large changes more than small changes, and
            we need an even function because if two terms \(\small{\delta_i}\) and \(\small{\theta_j}\) are equal in
            magnitude and opposite in sign, summing them cancels, so loss will be zero, which is not ideal. We need to
            measure the magnitude of loss.
        </p>
        <p>
            Suppose \(\small{x_i}\) is 3, \(\small{y_i}\) is 5, and \(\small{\theta_1}\) and \(\small{\theta_2}\) are 1
            and 6 respectively, the prediction is
        </p>
        <p class=”text-center”>
            \(\small{f(x_i, \theta)=\theta_1+\theta_2x_i=1+6(3)=19}\)
        </p>
        <p>
            then the error is
        </p>
        <p class=”text-center”>
            \(\small{\theta_i=(y_i-f(x_i, \theta))^2}\) \(\small{=(5-19)^2=(-14)^2=196}\)
        </p>
        <p>
            If the residual is doubled, the error quadruples.
        </p>
        <!--beware-->
        <br>
        <h2>
            Gradient Descent
        </h2>
        <p>
            Gradient descent is well-known technique to find the minimum (or maximum) of some objective function.
            Consider the loss function above.
            It is a function of two variables \(\small{\theta_1}\) and \(\small{\theta_2}\). Where \(\small{\theta}\)
            minimises \(\small{L}\), the gradient
            is zero (I'm assuming knowledge of what a gradient is, and maxima/minima in >2 dimensions). We can
            analytically solve for \(\small{\theta}\) for the above loss function because it's
            simple (we're not fitting a fancy curve, just a line).
        </p>
        <p class="text-center">
            \(\small{\nabla_{\theta} L=\begin{bmatrix}\dfrac{\partial L}{\partial \theta_1} \\ \\ \dfrac{\partial
            L}{\partial \theta_2}\end{bmatrix}=\textbf{0}}\)
        </p>
        <p>
            But suppose the function is complex. If it's not something simple like a linear equation \(\small{f(x_i,
            \theta)=\theta_1+\theta_2x_i}\), we may not be able to
            analytically solve for \(\small{\theta}\). Here, we find the approximate solution by carefully adjusting
            \(\small{\theta}\) until
            \(\small{L\approx}\) 0 and \(\small{\nabla_\theta L\approx \textbf{0}}\). This method of careful adjusting
            is gradient descent.
        </p>
        <div class="flex justify-center">
            <img class="responsive" src="../img/x2y2z.png" alt="paraboloid">
        </div>
        <p>
            Let's first understand two facts about the gradient.
        </p>
        <ul>
            <li>
                The gradient is in the direction of steepest ascent, and
            </li>
            <li>
                The gradient at a point is perpendicular to the level set through that point.
            </li>
        </ul>
        <p>
            The first point explains gradient descent. We initially start somewhere random on the graph. We compute the
            gradient at
            the point. Stepping along the gradient means we'll ascend the graph, stepping in the opposite direction
            means we'll descend.
            We multiply the gradient by some small constant (called learning rate \(\small{\eta}\)), and we subtract
            this from the original
            point to reach a new location. We keep doing this again and again until we reach a local minima where the
            gradient and loss are
            close to zero.
        </p>
        <div class="flex justify-center">
            <div class="overflow-auto">
                <p class="border-top border-bottom" style="white-space: nowrap;"><b>Algorithm</b> Gradient Descent</p>
                <p style="white-space: nowrap;">
                    initialise \(\small{\theta}\) randomly
                    <br>
                    until \(\small{L\approx 0}\)
                    <br>
                    <span class="padding-left">\(\small{\theta\coloneqq\theta-\eta\nabla_{\theta}L}\)</span>
                </p>
            </div>
        </div>
        <p>

        </p>
    </div>
</body>

</html>