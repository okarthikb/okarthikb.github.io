<!DOCTYPE html>

<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <link rel="stylesheet" href="../style.css">
  <link rel="preconnect" href="https://fonts.googleapis.com"> 
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin> 
  <link href="https://fonts.googleapis.com/css2?family=EB+Garamond&display=swap" rel="stylesheet">
  <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML"
    type="text/javascript"></script>
  <script src="https://cdn.jsdelivr.net/gh/google/code-prettify@master/loader/run_prettify.js"></script>
  <script>
    MathJax.Hub.Config({
      "HTML-CSS": {
        preferredFont: "TeX"
      }
    });
  </script>
  <title>Notes on RL</title>
</head>

<body>
  <div class="page">
    <p><a href="../index.html">‚Üê</a></p>
    <h2 style="border-bottom: 1px black solid;">Notes on reinforcement learning</h2>
    <br>
    <h3>Contents</h3>
    <p><a href="#intro"><u>What is reinforcment learning?</u></a></p>
    <p><a href="#functions"><u>Functions</u></a></p>
    <p><a href="#mdp"><u>MDP</u></a></p>
    <p><a href="#mc"><u>Monte-Carlo sampling</u></a></p>
    <p><a href="#sarsa"><u>SARSA</u></a></p>
    <p><a href="#ql"><u>\(Q\)-learning</u></a></p>
    <p><a href="#dql"><u>Deep \(Q\)-learning with experience replay</u></a></p>
    <p><a href="#pg"><u>Policy gradient</u></a></p>
    <br>
    <h3 id="intro">What is reinforcement learning?</h3>
    <p>
      We have an agent (Pacman, a robot, etc.) and an environment. The agent repeatedly interacts with the environment
      and receives reward signals.
      The agent adjusts its behavior using the information gained from interacting with the environment to achieve its
      goal: <i>maximize the
        cumulative reward.</i>
    </p>
    <p>
      Let's formalize some concepts. The agent looks at the current state \(s\in\mathcal{S}\), takes an action
      \(a\in\mathcal{A}\),
      receives a reward \(r\in\mathcal{R}\subset\mathbb{R}\) from the environment, and the environment transitions to
      the next state
      \(s'\in\mathcal{S}\). \(\mathcal{S}\) and \(\mathcal{A}\) can be discrete or continuous.
    </p>
    <div align="center" style="margin: 1em 0" class="overflow-scroll">
      <table>
        <tr>
          <th><b>Environment</b></th>
          <th>\(\mathcal{S}\)</th>
          <th>\(\mathcal{A}\)</th>
        </tr>
        <tr>
          <td>Robotics</td>
          <td>continuous</td>
          <td>continuous</td>
        </tr>
        <tr>
          <td>Pacman</td>
          <td>continous</td>
          <td>discrete</td>
        </tr>
        <tr>
          <td>?</td>
          <td>discrete</td>
          <td>continuous</td>
        </tr>
        <tr>
          <td>Blackjack</td>
          <td>discrete</td>
          <td>discrete</td>
        </tr>
      </table>
    </div>
    <br>
    <h3 id="functions">Functions</h3>
    <p>
      The trajectory \(\tau\) is defined as the sequence of state-action-reward transition tuples from the beginning to
      the end of an episode
    </p>
    <div class="overflow-scroll">
      <span>
        \[\tau=\{(s_0, a_0, r_0)\ldots(s_T, a_T, r_T)\}.\]
      </span>
    </div>
    <p>
      The <b>discounted</b> return after timestep \(t\) of a trajectory is defined as
    </p>
    <span>
      \[G_t(\tau)=\sum_{i=0}^{T-t-1}\gamma^ir_{i+t}\]
    </span>
    <p>
      where \(\gamma\in[0, 1]\) is the discount factor. \(\gamma\) measures how much the agent cares about long term
      returns. If the agent is playing Table Tennis, which doesn't involve a lot of long term planning, \(\gamma\)
      should be small. If it's trying to predict the stock market, which has a lot of short-term fluctuations,
      \(\gamma\)
      should be closer to 1 for long-term planning.
    </p>
    <p>
      The <b>policy</b> \(\pi\) of an agent is a function that tells what action to take given state. If \(\pi\) is
      deterministic, \(\pi(s)\) is the action to take given state \(s\). If \(\pi\) is stochastic, \(\pi(a\vert s)\) is
      the probability of taking action \(a\) given state \(s\).
    </p>
    <p>
      Since both the environment and policy can be stochastic, we care about maximizing the <i>expected</i> discounted
      return given state \(s\) for some policy \(\pi\) - we call this the State-Value (or just value) function
    </p>
    <div class="overflow-scroll">
      <span>
        \[V_{\pi}(s) = \mathbb{E}\big[G_t=\sum_{i=0}^{T-t-1}\gamma^ir_{i+t}\vert s\big]\]
      </span>
    </div>
    <p>
      because it gives the <b>value</b> of the state, i.e., what's the potential discounted return given state \(s\).
      The optimal policy \(\pi^*\) is the one that has the maximum value function
    </p>
    <div class="overflow-scroll">
      <span>
        \[\pi^*=\operatorname{argmax}_{\pi}(V_{\pi}(s)).\]
      </span>
    </div>
    <p>
      We have another function called the Action-Value (or just \(Q\)) function
    </p>
    <div class="overflow-scroll">
      <span>
        \[Q_{\pi}(s, a) = \mathbb{E}\big[G_t=\sum_{i=0}^{T-t-1}\gamma^ir_{i+t}\vert s, a\big]\]
      </span>
    </div>
    <p>
      and is similiar to the value function, except it's the expected discounted return given the agent took action
      \(a\) in current state \(s\). Again,
    </p>
    <span>
      \[\pi^*=\operatorname{argmax}_{\pi}(Q_{\pi}(s, a)).\]
    </span>
    <p>
      We also have what's called the <b>advantage</b> function
    </p>
    <span>
      \[A_{\pi}(s, a) = Q_{\pi}(s, a) - V_{\pi}(s)\]
    </span>
    <p>
      and it measures how much more or less than the expected discounted return will the agent recieve if it takes
      action \(a\) in state \(s\).
    </p>
    <br>
    <h3 id="mdp">MDP</h3>
    <p>
      Reinforcement problems can be framed as a <a
        href="https://en.wikipedia.org/wiki/Markov_decision_process"><u>Markov decision process</u></a> (MDP).
      A Markov decision process has a bunch of states and has the Markov property - the future statistics of the
      environment only depend on the current state and not
      the history of previous states. In each state, there is a set of possible actions. Each action has a different
      probability of being taken. Once an action is
      taken, there is a set of possible states the environment can transition to. The next state, like the action, is
      also sampled from a probability distribution.
    </p>
    <div align="center">
      <img src="../MDP.png" alt="MDP" class="height-auto border-05px-grey-solid">
    </div>
    <p>
      As we've seen in the previous section, \(\pi(a\vert s)\) is the probability of action \(a\) being taken given
      state \(s\). The state is also
      stochastic - \(P(s'\vert s, a)\) is the probability the environment will transition to next state \(s'\) given
      action \(a\) was taken in
      state \(s\). \(R(s, a)\) is the reward function - the immediate reward recieved by the agent when it takes action
      \(a\) in state \(s\).
      This leads to nice recursive relations
    </p>
    <div class="overflow-scroll">
      <span>
        \begin{eqnarray}
        V_{\pi}(s)&=&\mathbb{E}\big[G_t=\sum_{i=0}^{T-t-1}\gamma^ir_{i+t}\vert s\big]\\
        &=&\mathbb{E}\big[G_t=r_t+\gamma V_{\pi}(s')\vert s\big],
        \end{eqnarray}
      </span>
    </div>
    <p>
      and for the \(Q\) function
    </p>
    <div class="overflow-scroll">
      <span>
        \[Q_{\pi}(s, a) = \mathbb{E}\big[G_t=r_t+\gamma\mathbb{E}\big[Q(s', a)\big]\vert s, a\big].\]
      </span>
    </div>
    <p>
      Expanding the expectation, we get the Bellman expectation equations.
    </p>
    <div class="overflow-scroll">
      <span>
        \begin{eqnarray}
        V_{\pi}(s) &=& \sum_{a}Q_{\pi}(s, a)\pi(a\vert s),\\
        Q_{\pi}(s, a) &=& R(s, a) + \sum_{s'}V(s')P(s'\vert s, a),\\
        V_{\pi}(s) &=& \sum_{a}\bigl(R(s, a)+\sum_{s'}V(s')P(s'\vert s, a)\bigr)\pi(a\vert s),\\
        Q_{\pi}(s, a) &=& R(s, a) + \sum_{s'}\bigl(\sum_{a'}Q(s', a')\pi(a'\vert s')\bigr)P(s'\vert s, a),\\
        \end{eqnarray}
      </span>
    </div>
    <p>
      We can keep back-substituting and expanding the equation. But we don't need to do that. We are only interested in
      the optimal
      values, and not the expectation. If the agent follows the optimal policy \(\pi^*\), the following equations -
      called the
      Bellman optimality equations - hold
    </p>
    <div class="overflow-scroll">
      <span>
        \begin{eqnarray}
        V^*(s) &=& \operatorname{max}_a(Q(s, a)),\\
        Q^*(s, a) &=& R(s, a) + \operatorname{max}_{s'}(V^*(s')),\\
        V^*(s) &=& \operatorname{max}_a(R(s, a) + \gamma\sum_{s'}V^*(s')P(s'\vert s, a)),\\
        Q^*(s, a) &=& R(s, a) + \gamma\sum_{s'}\operatorname{max}_{a'}(Q^*(s', a'))P(s'\vert s, a)
        \end{eqnarray}
      </span>
    </div>
    <p>
      RL algorithms make use of the above equations. The types of RL algorithms are as follows.
    </p>
    <p>
      <b>Model-based</b>: The policy \(\pi\) relies on machine learning models like linear regression, random forest, XGboost, neural nets, etc.
      Learning happens in model-based RL.
    </p>
    <p>
      <b>Model-free</b>: No model is involved; the policy is deterministic. Planning happens in model-free RL.
    </p>
    <p>
      <b>On-policy</b>: The target policy \(\pi\) is trained using trajectories generated by itself.
    </p>
    <p>
      <b>Off-policy</b>: The target policy \(\pi\) is trained using trajectories generated by a different policy.
    </p>
    <br>
    <h3 id="mc">Monte-Carlo sampling</h3>
    <p>
      If we have the \(Q\) function, the optimal policy is just choosing the action that'll yield the maximum return
    </p>
    <span>
      \[\pi^*(s)=\operatorname{argmax}_a(Q^*(s, a)).\]
    </span>
    <p>
      How do we find \(Q\)? Well, whenever we deal with finding an expectation (remember that \(Q(s, a)\) is the expected
      return), we can
      use Monte-Carlo methods. Suppose the agent plays \(N\) episodes. It has a random policy, i.e., it's choosing
      actions at random. Let's
      say the agent took action \(a\) in state \(s\) at time step \(t_k\) in episode \(k\). The return recieved by the
      agent in each episode
      after visiting \(s\) and taking action \(a\) is \(G_{t_k}\) (by definition). The expected return is just the
      average of these returns as
      \(N\rightarrow \infty\)
    </p>
    <span>
      \begin{eqnarray}
      Q(s, a)&=&\lim_{N\rightarrow \infty}\frac{1}{N}\sum_{k=1}^NG_{t_k}\\
      &\approx&\frac{1}{N}\sum_{k=1}^NG_{t_k}.
      \end{eqnarray}
    </span>
    <p>
      The larger \(N\) is, the more accurate our approximation gets. We take what's called an \(\epsilon\)-greedy
      approach to train the agent.
      At each timestep, we take a random action with probability \(\epsilon\in[0, 1]\), and choose the action with
      maximum expected return
      with probability \(1-\epsilon\).
    </p>
    <div class="overflow-scroll">
      <span>
        \[
        \pi(s)=
        \begin{cases}
        \text{choose random}\;a && \text{w/ probability }\epsilon\\
        \operatorname{argmax}_a(Q(s, a)) && \text{w/ probability }1-\epsilon
        \end{cases}
        \]
      </span>
    </div>
    <p>
      This is called the exploration-exploitation trade-off. We want to explore new states by taking random actions but
      at the same time we want to
      encourage good behavior by using the partially trained policy and strengthening it.
    </p>
    <div class="codeblock" style="max-width: 290px;">
      <p><span class="blue">for</span> \((s, a)\in\mathcal{S}\times\mathcal{A}\)</p>
      <p class="indent">\(Q(s, a)\leftarrow\) arbitrary</p>
      <p class="indent">\(G(s, a)\leftarrow []\)</p>
      <p><span class="blue">for</span> \(N\) episodes</p>
      <p class="indent"><span class="grey">\\ generate a trajectory</span></p>
      <p class="indent">\(\tau\leftarrow \{(s_0, a_0, r_0)\ldots(s_T, a_T, r_T)\}\)</p>
      <p class="indent"><span class="blue">for</span> \((s, a, r)\) in \(\tau\)</p>
      <p class="indent"><span class="indent">\(G(s, a)\leftarrow\) \(r\) + return after \(s\)</span></p>
      <p class="indent"><span class="indent">\(Q(s, a)\leftarrow\) avg\((G(s, a))\)</span></p>
    </div>
    <p>
      We can implement the above pseudocode in Python. We use OpenAI's gym to train an agent to play <a
        href="https://github.com/openai/gym/blob/master/gym/envs/toy_text/blackjack.py"><u>Blackjack</u></a> using
      Monte-Carlo sampling. Import libraries first.
    </p>
    <div class="codeblock" style="max-width: 330px;">
      <pre class="prettyprint lang-python no-margin no-border"><code>import gym
from numpy.random import choice, rand
from numpy import argmax
from tqdm import tqdm</code></pre>
    </div>
    <p>
      Initialize the environment and hyperparameters.
    </p>
    <div class="codeblock" style="max-width: 400px;">
      <pre class="prettyprint lang-python no-margin no-border"><code>e = gym.make("Blackjack-v1")  # init environment
gamma = 1
N = 3000000  # number of episodes
eps = 0.7  # epsilon
Qs, Gs = {}, {}
# set of actions to take
actions = list(range(e.action_space.n))</code></pre>
    </div>
    <p>
      We create an empty \(Q\) and returns table.
    </p>
    <div class="codeblock" style="max-width: 480px;">
      <pre class="prettyprint lang-python no-margin no-border"><code># initialize Q table and returns dict
for sum in range(1, 22):
  for card in range(1, 11):
    for ace in [False, True]:
      Qs[(sum, card, ace)] = [0] * len(actions)
      # "G" - prev avg discounted return, i.e., prev Q(s, a)
      # "N" - # times (s, a) has been visited
      # new "G" - (prev * N + return) / (N + 1)
      Gs[(sum, card, ace)] = {"G": 0, "N": 0} </code></pre>
    </div>
    <p>
      Define the training loop.
    </p>
    <div class="codeblock" style="max-width: 500px;">
      <pre class="prettyprint lang-python no-margin no-border"><code>def train(N):
  for _ in tqdm(range(N)):
    d = False
    s = e.reset()
    tau, G = [], []
    # generate trajectory
    while not d:
      # eps-greedy policy
      a = choice(actions) if rand() < eps else argmax(Qs[s])
      tau.append((s, a))  # store (s, a) in tau
      s, r, d, _ = e.step(a)  # take action
      G.append(r)  # store r to calc discounted return
    # calc discounted return
    for t in range(len(G) - 2, -1, -1):
      G[t] += gamma * G[t + 1]
    # train
    for (s, a), g in zip(tau, G):
      # calc new avg, i.e., new approx expectation
      Gs[s]["G"] = (Gs[s]["N"] * Gs[s]["G"] + g) / (Gs[s]["N"] + 1)
      Gs[s]["N"] += 1
      # update Q
      Qs[s][a] = Gs[s]["G"]</code></pre>
    </div>
    <p>
      We then compare the learned policy with a random policy (choosing random actions).
    </p>
    <div class="codeblock" style="max-width: 480px;">
      <pre class="prettyprint lang-python no-margin no-border"><code># run an episode w/ or w/o policy
def play(policy=False):
  d = False
  s = e.reset()
  G = 0
  while not d:
    a = argmax(Qs[s]) if policy else choice(actions)
    s, r, d, _ = e.step(a)
    G += r
  return G


# count # wins, draws, and losses in 100 games
def test(policy=False):
  G = [play(policy) for _ in range(100)]
  wins, draws, losses = 0, 0, 0
  for g in G:
    if g == -1: losses += 1
    elif g == 0: draws += 1
    else: wins += 1
  print(f"wins: {wins}\tdraws: {draws}\tlosses: {losses}")


test()  # random policy

train(N)

test(True)  # using Q table</code></pre>
    </div>
    <p>
      The agent improves somewhat. First line is score before training, next line is after.
    </p>
    <div class="codeblock" style="max-width: 300px;">
      <pre class="no-margin no-border"><code>wins: 28    draws: 4    losses: 68    
wins: 43    draws: 8    losses: 49</code></pre>
    </div>
    <br>
    <h3 id="sarsa">SARSA</h3>
    <p>
      SARSA stands for "State-Action-Reward-State-Action". It's an on-policy model-free algorithm. Here too we find the
      \(Q\) table, but we use
      the recurrence relation
    </p>
    <span>
      \[Q(s, a)=R(s, a)+Q(s', a')\]
    </span>
    <p>
      instead of using random trajectories and calculating an expectation. We need to find a \(Q\) table that satisfies
      the above relation. How
      do we do this? We do this by computing how wrong \(Q(s, a)\) at every timestep and the correcting it slightly.
      Overtime, the error
      reduces, and we'll have a good \(Q\) table. How wrong \(Q(s, a)\) is measured by what's called the temporal
      difference error or TD error
      (well, it's a correction term)
    </p>
    <span>
      \[\delta=R(s, a)+Q(s', a')-Q(s, a)\]
    </span>
    <p>
      and we adjust \(Q\) by scaling the above correction term and adding it to \(Q\)
    </p>
    <span>
      \[Q(s, a)\leftarrow Q(s, a)+\eta\delta\]
    </span>
    <p>
      where \(\eta\) is the learning rate. The larger \(\eta\) is, the larger the correction, but the greater the
      likelihood it's off from the ideal
      correction term. The pseudocode is like so.
    </p>
    <div class="codeblock" style="max-width: 400px;">
      <p><span class="blue">for</span> \((s, a)\in\mathcal{S}\times\mathcal{A}\)</p>
      <p class="indent">\(Q(s, a)\leftarrow\) arbitrary</p>
      <p><span class="blue">for</span> \(N\) episodes</p>
      <p class="indent">\(s\leftarrow\) initial state</p>
      <p class="indent">\(a\leftarrow \pi(s)\)</p>
      <p class="indent"><span class="blue">until</span> episode done</p>
      <p class="indent"><span class="indent"></span>take action \(a\), receive next state \(s'\) and reward \(r\)</p>
      <p class="indent"><span class="indent"></span>\(a'\leftarrow \pi(s')\)</p>
      <p class="indent"><span class="indent"></span>\(\delta\leftarrow r + Q(s', a') - Q(s, a)\)</p>
      <p class="indent"><span class="indent"></span>\(Q(s, a)\leftarrow Q(s, a) + \eta\delta\)</p>
      <p class="indent"><span class="indent"></span>\(s\leftarrow s'\)</p>
      <p class="indent"><span class="indent"></span>\(a\leftarrow a'\)</p>
    </div>
    <p>
      Again, we implement this for Blackjack. Only the training loop and initialization are different here.
    </p>
    <div class="codeblock" style="max-width: 390px;">
      <pre class="prettyprint lang-python no-margin no-border"><code>e = gym.make("Blackjack-v1")
eta = 3e-2  # learning rate
N = 200000  # number of episodes
eps = 0.6  # epsilon
Q = {}
actions = list(range(e.action_space.n))


# initialize Q table
for sum in range(1, 33):
  for card in range(1, 11):
    for ace in [False, True]:
      Q[(sum, card, ace)] = rand(len(actions))</code></pre>
    </div>
    <p>
      The training loop is much simpler than MC sampling.
    </p>
    <div class="codeblock" style="max-width: 520px;">
      <pre class="prettyprint lang-python no-margin no-border"><code>def train(N):
  for _ in tqdm(range(N)):
    d = False
    s = e.reset()  # initial state
    # first action
    a = choice(actions) if rand() < eps else argmax(Q[s])
    while not d:
      s_, r, d, _ = e.step(a)  # take action
      # get next action
      a_ = choice(actions) if rand() < eps else argmax(Q[s_])
      delta = r + Q[s_][a_] - Q[s][a]  # compute TD error
      Q[s][a] += eta * delta  # update Q
      a = a_  # next action to take</code></pre>
    </div>
    <p>
      The results are much better for lower training time. SARSA is the OG RL algorithm.
    </p>
    <div class="codeblock" style="max-width: 300px;">
      <pre class="no-margin no-border"><code>wins: 29    draws: 6    losses: 65
wins: 42    draws: 7    losses: 51</code></pre>
    </div>
    <br>
    <h3 id="ql">\(Q\)-learning</h3>
    <p>
      \(Q\)-learning is very similiar to SARSA. Here, the recursive relation we care about is the Bellman optimality
      equation for \(Q\)
    </p>
    <div class="overflow-scroll">
      <span>
        \[Q^*(s, a)=R(s, a)+\operatorname{max}_a(Q(s', a)).\]
      </span>
    </div>
    <p>
      The \(Q\) table for the optimal policy \(\pi^*\) will satisfy the above relation, and so we try to approximate
      \(Q^*\) by using
      the TD error
    </p>
    <div class="overflow-scroll">
      <span>
        \[\delta=R(s, a) + \operatorname{max}_a(Q(s')) - Q(s, a)\]
      </span>
    </div>
    <p>
      and making the update (just like SARSA)
    </p>
    <span>
      \[Q(s, a)\leftarrow Q(s, a) + \eta\delta.\]
    </span>
    <p>
      \(Q\)-learning is model-free and off-policy, unlike SARSA which is on-policy. Why? Recall the difference between
      off and on-policy. The
      update in SARSA is made by taking the next action according to an \(\epsilon\)-greedy policy, the same policy used
      to generate states.
      The update in \(Q\)-learning doesn't depend on the \(\epsilon\)-greedy policy (i.e., it doesn't depend on action
      taken in next state) with
      which we generate the states. We're simply picking the maximum \(Q\) value for the next state. With this, the
      pseudocode is as follows.
    </p>
    <div class="codeblock" style="max-width: 410px;">
      <p><span class="blue">for</span> \((s, a)\in\mathcal{S}\times\mathcal{A}\)</p>
      <p class="indent">\(Q(s, a)\leftarrow\) arbitrary</p>
      <p><span class="blue">for</span> \(N\) episodes</p>
      <p class="indent">\(s\leftarrow\) initial state</p>
      <p class="indent"><span class="blue">until</span> episode done</p>
      <p class="indent"><span class="indent">\(a\leftarrow \pi(s)\)</span></p>
      <p class="indent"><span class="indent">take action \(a\), receive next state \(s'\) and reward \(r\)</span></p>
      <p class="indent"><span class="indent">\(\delta\leftarrow r + \operatorname{max}_a(Q(s')) - Q(s, a)\)</span></p>
      <p class="indent"><span class="indent">\(Q(s, a)\leftarrow Q(s, a) + \eta\delta\)</span></p>
      <p class="indent"><span class="indent">\(s\leftarrow s'\)</span></p>
    </div>
    <p>
      The pseudocode looks very similiar! We have only made slight modifications to the training loop.
    </p>
    <div class="codeblock" style="max-width: 500px;">
      <pre class="prettyprint lang-python no-margin no-border"><code>def train(N):
  for _ in tqdm(range(N)):
    d = False
    s = e.reset()  # initial state
    while not d:
      # get action
      a = choice(actions) if rand() < eps else argmax(Q[s])
      s_, r, d, _ = e.step(a)  # take action
      delta = r + max(Q[s_]) - Q[s][a]  # compute TD error
      Q[s][a] += eta * delta  # update Q</code></pre>
    </div>
    <p>
      For the same hyperparameters, it performs similiarly to SARSA. Blackjack isn't super complex, so we may not see
      differences.
    </p>
    <div class="codeblock" style="max-width: 300px;">
      <pre class="no-margin no-border"><code>wins: 32    draws: 7    losses: 61
wins: 43    draws: 3    losses: 54</code></pre>
    </div>
    <p>
      We've gone through some model-free RL algorithms. Now we'll take a look at model-based RL algorithms.
    </p>
    <br>
    <h3 id="dql">Deep \(Q\)-learning with experience replay</h3>
    <p>
      Blackjack's state space is quite small. There are only 32 x 10 x 2 = 640 states. What if the state space is
      extremely large?
      What if it's continuous? Consider a video game like Pong. There is no way we can enumerate all possible states and
      make a \(Q\) table. It's
      memory intensive (assuming a 100x100 screen and suppose each pixel has 2 possible values (black and white) -
      that's
      \(2^{10000}\approx10^{3010}\) states!), and computationally intractable (if the agent follows an \(\epsilon\)-greedy
      policy, the state space will
      be too large to explore in reasonable time).
    </p>
    <p>
      Since \(Q\)-learning is model-free, the agent cannot reason about the environment. 
      The agent cannot predict rewards because it doesn't have a model
      of the environment. If it had,
      then we don't need to enumerate over all the states. We can arrive at a good approximation in fewer episodes.
    </p>
    <p>
      Enter model-based learning. The three functions - \(\pi\), \(Q_{\pi}(s, a)\), and \(V_{\pi}(s)\) - are functions
      of the state (note that in
      model-free learning, \(\pi\), \(Q_{\pi}\), and \(V_{\pi}\) are just lookup tables - not functions with parameters)
    </p>
    <div class="overflow-scroll">
      <span>
        \begin{eqnarray}
        V_{\pi}(s) &=& f_u(s),\\
        Q_{\pi}(s, a) &=& g_v(s, a),\\
        \pi(s) &=& h_w(s)
        \end{eqnarray}
      </span>
    </div>
    <p>
      where \(f_u\) means \(f\) is some function parametrized by \(u\). Suppose we want to model the value function.
      Then we'll run an episode,
      calculate the discounted return, and use that as the target value or label. We then use machine learning models -
      regression, XGboost, SVMs,
      neural nets, etc. - to predict the target values.
    </p>
    <div align="center">
      <img src="../model_based.png" alt="MBRL" class="height-auto border-05px-grey-solid">
    </div>
    <p>
      In supervised learning, we have a dataset with input and labels. In reinforcement
      learning, we generate the dataset by letting the agent finish an episode. The targets are calculated using the
      recursive relations discussed in
      the MDP section. Suppose we want to approximate the value function for a policy \(\pi\). The value function obeys
      the relation
    </p>
    <div class="overflow-scroll">
      <span>
        \[V_{\pi}(s)=R(s, a)+\gamma V_{\pi}(s').\]
      </span>
    </div>
    <p>
      RHS is the target. The TD error is just the target minus the predicted value (we generate the target using the
      same model we're training
      due to the recursive relationship above - this is how RL is different from supervised learning)
    </p>
    <span>
      \begin{eqnarray}
      y &=& R(s, a) + f_u(s'),\\
      \delta &=& y - f_u(s).
      \end{eqnarray}
    </span>
    <p>
      The MSE error or loss is just
    </p>
    <span>
      \[L(u)=\frac{1}{2}\delta^2\]
    </span>
    <p>
      and we can minimize this loss by using gradient descent. The nice property we have is that although \(f_u\) is
      parametrized randomly at the
      start, it converges close enough to the correct value function given we train it on enough states. I won't go over
      the proof here. Instead
      of using the recursive relationship above to generate the targets, one can directly train the agent on returns, so
      the TD error is
    </p>
    <span>
      \begin{eqnarray}
      y &=& G_t,\\
      \delta &=& y - f_u(s).
      \end{eqnarray}
    </span>
    <p>
      This is kinda not ideal, because the policy (and maybe state) is stochastic, and the agent will receive different
      returns in different episodes
      for the same state. But if the state space is large, the chances of visiting the same state diminishes, so it's
      not too bad. The pseudocode
      for approximating the value function for some policy \(\pi=h_w\) looks like so.
    </p>
    <div class="codeblock" style="max-width: 400px;">
      <p>\(f_u\leftarrow\) initialize with arbitrary \(u\)</p>
      <p><span class="blue">for</span> \(N\) episodes</p>
      <p>\(s\leftarrow\) initial state</p>
      <p class="indent"><span class="blue">until</span> episode done</p>
      <p class="indent"><span class="indent">\(a\leftarrow h_w(s)\)</span></p>
      <p class="indent"><span class="indent">take action \(a\), receive next state \(s'\) and reward \(r\)</span></p>
      <p class="indent"><span class="indent">\(y\leftarrow r + f_u(s')\)</span></p>
      <p class="indent"><span class="indent">\(\delta\leftarrow y - f_u(s)\)</span></p>
      <p class="indent"><span class="indent">\(du\leftarrow -\delta\nabla_uf_u\)</span></p>
      <p class="indent"><span class="indent">\(u\leftarrow u-\eta du\)</span></p>
    </div>
    <p>
      The policy is parametrized by \(w\). Adjusting the parameters \(w\) of \(h_w\) means we get a different policy.
      Instead of using a model
      to approximate \(\pi\), we can model \(Q\). This is what deep \(Q\)-learning is. Just like \(Q\)-learning, we find
      the \(Q\)-function, but
      this time, we're trying to approximate it using a model (usually a neural net) based on state. The policy is just
      choosing the action with
      maximum expected reward
    </p>
    <div class="overflow-scroll">
      <span>
        \begin{eqnarray}
        Q(s, a) &=& g_v(s, a),\\
        \pi(s) &= & \operatorname{argmax}_a(g_v(s, a)).
        \end{eqnarray}
      </span>
    </div>
    <p>
      Since we're using a neural net, we slightly modify \(g_{\theta}\) (\(\theta\) being parameters of the net) - it
      depends only on \(s\) and
      not on \(a\). The neural net outputs a vector of expected returns for taking different actions give state \(s\).
    </p>
    <div align="center">
      <img src="../dqn.png" alt="DQN" class="height-auto border-05px-grey-solid">
    </div>
    <p>
      So the agent policy looks more like
    </p>
    <div class="overflow-scroll">
      <span>
        \[\pi(s)=\operatorname{argmax}_a(g_{\theta}(s)_a).\]
      </span>
    </div>
    <p>
      This idea of combining \(Q\)-learning with deep neural nets (specifically CNNs) was what DeepMind got acquired for by Google in 2015.
      They introduced this in this <a href="https://arxiv.org/abs/1312.5602"><u>paper</u></a>. Another idea was experience replay.
      Notice that in
      \(Q\)-learning, we take consecutive sequence of states and do training? Consecutive sequences are highly
      correlated, so the agent
      may become biased. So instead of training with a single state at each timestep, we store
      the state
      transitions \((s, a, r, s')\) in memory (called replay memory \(D\)), and sample a batch of transitions from \(D\)
      at every
      timestep and do training. Using experience replay decorrelates sequences of states.
    </p>
    <p>
      Let's just denote \(g_{\theta}\) as \(Q_{\theta}\). \(Q\) referred to the lookup table, and \(g\) denoted the
      function
      that approximates \(Q\), but for brevity, they're the same. The optimal \(Q\) function \(Q^*\) has the property
      (from the Bellman
      optimality equations)
    </p>
    <div class="overflow-scroll">
      <span>
        \[Q^*(s, a)=R(s, a)+\operatorname{max}_a(Q^*(s', a)).\]
      </span>
    </div>
    <p>
      But \(Q_{\theta}\) is a neural net that returns a vector of expected returns for various actions. We need the
      optimal \(\theta\),
      \(\theta^*\), that satisfies the above Bellman relation in the neural net case
    </p>
    <div class="overflow-scroll">
      <span>
        \[Q_{\theta^*}(s)_a=R(s, a)+\operatorname{max}_a(Q_{\theta^*}(s')_a)\]
      </span>
    </div>
    <p>
      where \(Q_{\theta}\) returns a vector of expected rewards and \(Q_{\theta}(s)_a=Q(s, a)\), i.e., the
      \(a^{\text{th}}\) element of the
      vector denoting action \(a\). The \(\epsilon\)-greedy policy is just
    </p>
    <div class="overflow-scroll">
      <span>
        \[\pi(s)=
        \begin{cases}
        \text{choose random}\;a && \text{w/ probability }\epsilon\\ \operatorname{argmax}_a(Q_{\theta}(s)_a) &&
          \text{w/ probability }1-\epsilon\\ \end{cases} \] 
      </span>
    </div>
    <p>
      We don't always pass the current state exactly. We sometimes pre-process sequences of previous-states and pass
      that into the \(Q\) network. The pseudocode for training is as follows.
    </p>
    <div class="codeblock" style="max-width: 420px;">
      <p>\(D\leftarrow\) initialize replay memory with capacity \(C\)</p>
      <p>\(Q_{\theta}\leftarrow\) initialize \(Q\) with arbitrary \(\theta\)</p>
      <p><span class="blue">for</span> \(N\) episodes</p>
      <p class="indent">\(s\leftarrow\) initial state</p>
      <p class="indent">\(s\leftarrow \phi(s)\)<span class="grey">&nbsp \\ pre-process state</p>
      <p class="indent"><span class="blue">until</span> episode done</p>
      <p class="indent"><span class="indent">\(a\leftarrow \pi(s)\)</span>&nbsp <span class="grey">\\ choose action
          acc. \(\epsilon\)-greedy</span></p>
      <p class="indent"><span class="indent">take action \(a\), receive next state \(s'\) and reward \(r\)</span>
      </p>
      <p class="indent"><span class="indent">\(s'\leftarrow \phi(s')\)</span></p>
      <p class="indent"><span class="indent"><span class="blue">append</span> \((s, a, r, s')\) to \(D\)</span></p>
      <p class="indent"><span class="indent">\(s\leftarrow s'\)</span></p>
      <p class="indent"><span class="indent">\(B\leftarrow\) sample random batch from \(D\)</span></p>
      <p class="indent"><span class="indent"><span class="blue">for</span> \((s, a, r, s')\in B\)</span></p>
      <p class="indent"><span class="indent"><span class="indent">\(y\leftarrow\begin{cases}r && \text{s is
            terminal} \\ r + \operatorname{max}_a(Q_{\theta}(s')_a && \text{otherwise}\end{cases}\)</span></span></p>
      <p class="indent"><span class="indent"><span class="indent">\(\delta\leftarrow y -
            Q_{\theta}(s)_a\)</span></span></p>
      <p class="indent"><span class="indent"><span class="indent">\(d\theta\leftarrow
            -\delta\nabla_{\theta}Q_{\theta}(s)_a\)<span class="grey">&nbsp \\ backprop here</span></span></span>
      </p>
      <p class="indent"><span class="indent"><span class="indent">\(\theta\leftarrow \theta-\eta
            d\theta\)</span></span></p>
    </div>
    <p>
      Let's implement this in Python using PyTorch. The agent will learn to play <a
        href="https://gym.openai.com/envs/CartPole-v0/"><u>CartPole</u></a>.
      Import necessary libraries, define hyperparameters, and initialize Q network and environment.
    </p>
    <div class="codeblock" style="max-width: 380px;">
      <pre class="prettyprint lang-python no-margin no-border"><code>import gym
import time
import torch
import random
import collections
import numpy as np
import matplotlib.pyplot as plt


# define the Q network
class network(torch.nn.Module):
  def __init__(self):
    super(network, self).__init__()
    # define layers
    self.fc = torch.nn.Linear(4, 256)
    self.out = torch.nn.Linear(256, 2)

  # forward pass
  def forward(self, x):
    x = torch.nn.functional.relu(self.fc(x))
    x = torch.nn.functional.relu(self.out(x))
    return x


# initialize the network
Q = network()

# initialize the environment
e = gym.make("CartPole-v0")

# hyperparameters
N = 100  # episodes
eps = 0.9  # epsilon
# decay epsilon as agent gets better
decay = 0.99
gamma = 0.99999
lr = 1e-3  # learing rate eta
capacity = 2000  # replay memory capacity
# size of sample to take from replay memory
batch_size = 256

# define optimizer
opt = torch.optim.Adam(Q.parameters(), lr=lr)</code></pre>
    </div>
    <p>
      We define the state pre-processing function \(\phi\).
    </p>
    <div class="codeblock" style="max-width: 380px;">
      <pre class="prettyprint lang-python no-margin no-border"><code>def phi(s):
  return torch.tensor(s, dtype=torch.float32)</code></pre>
    </div>
    <p>
      We want to test out our \(Q\) network. We define a simple loop to calculate episode return.
    </p>
    <div class="codeblock" style="max-width: 270px;">
      <pre class="prettyprint lang-python no-margin no-border"><code>def play():
  d = False
  s = e.reset()
  G = 0
  with torch.no_grad():
    while not d:
      s = phi(s)
      a = np.argmax(Q(s)).item()
      s, r, d, _ = e.step(a)
      G += r
  return G</code></pre>
    </div>
    <p>
      Now we define the training loop.
    </p>
    <div class="codeblock" style="max-width: 440px;">
      <pre class="prettyprint lang-python no-margin no-border"><code>def train():
  global Q, eps, opt
  # intiailize empty replay memory
  D = collections.deque(maxlen=capacity)
  G = []  # store returns
  # main loop
  for i in range(1, N + 1):
    d = False
    s = phi(e.reset())
    # run episode
    while not d:
      # choose action acc. eps-greedy
      if random.random() < eps:
        a = e.action_space.sample()
      else:
        with torch.no_grad():
          a = torch.argmax(Q(s)).item()
      # take action
      ns, r, d, _ = e.step(a)
      ns = phi(ns)
      # store transition in replay memory
      D.append((s, a, r, ns, d))
      s = ns
      # sample batch of transitions from D
      if len(D) <= batch_size:
        B = D
      else:
        B = random.sample(D, batch_size)
      # for each transition in B
      for (s_, a_, r_, ns_, d_) in B:
        opt.zero_grad()
        # compute target
        with torch.no_grad():
          if d_:
            # target val
            y = torch.tensor(r_, dtype=torch.float32) 
          else:
            y = r_ + gamma * torch.max(Q(ns_))
        Qs = Q(s_)  # predicted Q vals
        loss = (y - Qs[a_]) ** 2  # compute loss
        loss.backward()  # backprop
        opt.step()  # update params
    # play a game w/ new Q and store return
    G.append(play())
    eps *= decay  # decay eps
    # log once every 10 episodes
    if i % 10 == 0:
      print(f"episode: {i}\treturn: {G[-1]}")
  # return returns
  return G</code></pre>
    </div>
    <p>
      Train the net, save the model and returns plot.
    </p>
    <div class="codeblock" style="max-width: 290px;">
      <pre class="prettyprint lang-python no-margin no-border"><code>G = train()

# save the trained model
torch.save(Q.state_dict(), "Q.pt")

# plot returns
fig = plt.figure(figsize=(8, 8))
plt.xlabel("episode")
plt.ylabel("return")
plt.plot(np.arange(N), G)
# save plot
plt.savefig("returns.png")</code></pre>
    </div>
    <p>
      Output should look like so.
    </p>
    <div class="codeblock" style="max-width: 250px;">
      <pre class="no-margin no-border"><code>episode: 10	return: 9.0
episode: 20	return: 30.0
episode: 30	return: 200.0
episode: 40	return: 200.0
episode: 50	return: 200.0
episode: 60	return: 200.0
episode: 70	return: 200.0
episode: 80	return: 200.0
episode: 90	return: 141.0
episode: 100	return: 105.0</code></pre>
    </div>
    <p>
      As we can see, the agent quickly improves. By episode 30, it's getting the maximum possible return. As
      training goes on, it starts
      to overfit and we see a reduction in return.
    </p>
    <div align="center">
      <img src="../dql_returns.png" alt="returns" class="height-auto border-05px-grey-solid">
    </div>
    <p>
      Agent balacing pole after training.
    </p>
    <div align="center">
      <img src="../qnet_after.gif" alt="returns" class="height-auto border-05px-grey-solid">
    </div>
    <br>
    <h3 id="pg">Policy gradient</h3>
    <p>
      Instead of training a neural net to approximate the \(Q\) function, we can directly train a policy network.
      The policy network \(\pi_{\theta}\)
      returns the probability distribution of actions to take given state. The action to take is sampled from this
      distribution. The simple idea is
      to increase the probability of actions in proportion to the return. We hope that by doing this for many
      episodes, the favorable
      actions get encouraged more than unfavorable ones.
    </p>
    <p>
      The goal is to maximize the expected return of trajectory \(\tau\) given policy \(\pi_{\theta}\)
    </p>
    <div class="overflow-scroll">
      <span>
        \[J(\theta)=\mathbb{E}\big[G(\tau)\big]=\int_{\tau}G(\tau)p(\tau)\tag{1}\]
      </span>
    </div>
    <p>
      where the optimal \(\theta\) is
    </p>
    <span>
      \[\theta^*=\operatorname{argmax}_{\theta}(J(\theta))\]
    </span>
    <p>
      and we find \(\theta^*\) through gradient <b>ascent</b> (reach local maxima instead of minima)
    </p>
    <div class="overflow-scroll">
      <span>
        \[\theta\leftarrow \theta + \eta\nabla_{\theta}J(\theta).\]
      </span>
    </div>
    <p>
      But what <b>is</b> \(\nabla_{\theta}J(\theta)\)? We'll derive it. Consider the integral above. By definition,
      the expected return is the sum
      over all \(\tau\) of the return from \(\tau\) - \(G(\tau)\) - times the probability of the trajectory -
      \(p(\tau)\) - given policy \(\pi_{\theta}\).
      What's \(p(\tau)\)? By definition, the probability the agent takes action \(a\) given state \(s\) is
      \(\pi_{\theta}(a\vert s)\) at each
      timestep \(t\). Once the agent took action \(a\) in state \(s\), if the environment is also stochastic, the
      probability of it
      transitioning to some next state \(s'\) is \(P(s'\vert s, a)\). So the probability of the transition \((s_t,
      a_t, r_t, s_{t + 1})\) at
      timestep \(t\) is
    </p>
    <div class="overflow-scroll">
      <span>
        \[p(\text{transition})=P(s_{t+1}\vert s_t, a_t)\pi_{\theta}(a_t\vert s_t).\]
      </span>
    </div>
    <p>
      The probability of the trajectory \(\tau\) is just the product of probabilities of transitions
    </p>
    <div class="overflow-scroll">
      <span>
        \[p(\tau)=\prod_{t=0}^{T-1}P(s_{t+1}\vert s_t, a_t)\pi_{\theta}(a_t\vert s_t).\tag{2}\]
      </span>
    </div>
    <p>
      The return of a trajectory \(G(\tau)\) is just the discounted sum of rewards at each timestep
    </p>
    <div class="overflow-scroll">
      <span>
        \[G(\tau)=\sum_{t=0}^{T-1}\gamma^tr_t.\tag{3}\]
      </span>
    </div>
    <p>
      Since we care about the gradient of \(J\) w.r.t \(\theta\), we only care about the terms that are dependent on
      \(\theta\),
      which is \(p(\tau)\) (because it contains \(\pi_{\theta}\)). We make use of the following trick called the-log
      trick for simplification
    </p>
    <div class="overflow-scroll">
      <span>
        \[\nabla_{\theta}\log(f(\theta)) = \frac{\nabla_{\theta}f(\theta)}{f(\theta)} \implies
        \nabla_{\theta}f(\theta) = f(\theta)\nabla_{\theta}\log(f(\theta)).\]
      </span>
    </div>
    <p>
      We now calculate \(\nabla_{\theta}J(\theta)\).
    </p>
    <div class="overflow-scroll">
      <span>
        \begin{eqnarray}
        \nabla_{\theta}J(\theta) &=& \nabla_{\theta}\int_{\tau}G(\tau)p(\tau)\\
        &=& \int_{\tau}G(\tau)\nabla_{\theta}p(\tau)\\
        &=& \int_{\tau}(G(\tau)\nabla_{\theta}\log(p(\tau)))p(\tau)\\
        &=& \mathbb{E}\big[G(\tau)\nabla_{\theta}\log(p(\tau))\big]\\
        &\approx& \frac{1}{N}\sum_{k=1}^NG(\tau_k)\nabla_{\theta}\log(p(\tau_k)).\tag{4}
        \end{eqnarray}
      </span>
    </div>
    <p>
      This result is called as the <b>policy gradient theorem</b>. We simplify \(\nabla_{\theta}\log(p(\tau_k))\) by
      substituting (2) in (4) like so
    </p>
    <div class="overflow-scroll">
      <span>
        \begin{eqnarray}
        \nabla_{\theta}\log(p(\tau_k)) &=& \nabla_{\theta}\log\bigl(\prod_{t=0}^{T_k-1}P(s_{t + 1}\vert s_t,
        a_t)\pi_{\theta}(a_t\vert s_t)\bigr)\\
        &=& \nabla_{\theta}\sum_{t=0}^{T_k-1}\log(P(s_{t + 1}\vert s_t, a_t)\pi_{\theta}(a_t\vert s_t))\\
        &=& \sum_{t=0}^{T_k-1}\nabla_{\theta}(\log(P(s_{t + 1}\vert s_t, a_t)) + \log(\pi_{\theta}(a_t\vert s_t)))\\
        &=& \nabla_{\theta}\sum_{t=0}^{T_k-1}\log(\pi_{\theta}(a_t\vert s_t)).\tag{5}
        \end{eqnarray}
      </span>
    </div>
    <p>
      Substituting (5) in (4) finally gives us the gradient of the objective function! This gradient is what we
      called the policy gradient. It's the
      gradient of the objective function w.r.t to the parameters of the policy function \(\pi_{\theta}\) (which is
      usually a deep neural net)
    </p>
    <div class="overflow-scroll">
      <span>
        \[\nabla_{\theta}J(\theta)\approx\nabla_{\theta}\frac{1}{N}\sum_{k=1}^NG(\tau_k)\sum_{t=0}^{T_k-1}\log(\pi_{\theta}(a_{k, t}\vert
        s_{k, t})).\]
      </span>
    </div>
    <p>
      We write this in an alternate form that's better for training. Instead of multiplying the entire inner sum by
      the total return, we multiply
      each term in the inner sum by the return after timestep \(t\)
    </p>
    <div class="overflow-scroll">
      <span>
        \[\nabla_{\theta}J(\theta)\approx\nabla_{\theta}\frac{1}{N}\sum_{k=1}^N\sum_{t=0}^{T_k-1}G_t(\tau_k)\log(\pi_{\theta}(a_{k, t}\vert
        s_{k, t})).\]
      </span>
    </div>
    <p>
      Intuitively, the above formula makes sense. We want to encourage an action in proportion to the return after
      the action was taken in some state,
      and not in proportion to the total trajectory return. The formal proof can be found in this
      <a href="https://danieltakeshi.github.io/2017/03/28/going-deeper-into-reinforcement-learning-fundamentals-of-policy-gradients/"><u>blog</u></a>.
    </p>
    <p>
      The formula may look hairy, but the objective is very simple to calculate (that's what all the derivation
      above was for -
      to make it easier to compute the objective function, which is the expected return as a function of
      \(\theta\)). Just a single for loop. And
      we have automatic differentiation in PyTorch, so calling <code>J.backward()</code> after storing objective in
      <code>J</code> computes the gradients for us. Neat! The vanilla policy gradient algorithm is as follows.
    </p>
    <div class="codeblock" style="max-width: 370px;
    ">
      <p>\(\pi_{\theta}\leftarrow\) initialize policy network with arbitrary \(\theta\)</p>
      <p><span class="blue">for</span> \(K\) epochs</p>
      <p class="indent">generate \(N\) trajectories \(\{\tau_1\ldots\tau_N\}\)</p>
      <p class="indent">\(J\leftarrow \frac{1}{N}\sum_{k=1}^N\sum_{t=0}^{T_k-1}G_t(\tau_k)\log(\pi_{\theta}(a_{k, t}\vert s_{k, t}))\)</p>
      <p class="indent">\(\theta\leftarrow \theta+\eta\nabla_{\theta}J\)</p>
    </div>
    <p>
      Smol n' cute, but can learn a lot of games! This is a classic model-based RL algorithm that directly implements the core idea of RL - take more actions that yield higher reward. Let's
      implement this. We initialize the policy network the same way we did \(Q\)-network, but the output layer is softmax (because it's a probability distribution).
    </p>
    <div class="codeblock" style="max-width: 440px;">
      <pre class="prettyprint lang-python no-margin no-border"><code># define the policy network
class network(torch.nn.Module):
  def __init__(self):
    super(network, self).__init__()
    # define layers
    self.fc = torch.nn.Linear(4, 512)
    self.out = torch.nn.Linear(512, 2)

  # forward pass
  def forward(self, x):
    x = torch.nn.functional.relu(self.fc(x))
    x = torch.nn.functional.softmax(self.out(x), -1)
    return x
  

pi = network().to(torch.device("cpu"))</code></pre>
    </div>
    <p>
      Initialize hyperparameters.
    </p>
    <div class="codeblock" style="max-width: 350px;">
      <pre class="prettyprint lang-python no-margin no-border"><code># initialize the environment
e = gym.make("CartPole-v0")
actions = [0, 1]

# hyperparameters
epochs = 100  # episodes
N = 10
gamma = 1
lr = 3e-3  # learing rate eta
frames = 1

# define optimizer
opt = torch.optim.Adam(pi.parameters(), lr=lr)</code></pre>
    </div>
    <p>
      If we're training say, an Atari agent, a single frame may not contain information like velocity, or fire-rate, or whatever. So the policy network (or \(Q\) network if we're doing \(Q\)-learning)
      will receive a pre-processed sequence of the previous \(k\) frames. We store the previous \(k\) frames in a Python <code>collections.deque</code> called <code>seq</code>.
      For CartPole-v0, the sequence length is just 1. We have enough info in a single frame.
    </p>
    <div class="codeblock" style="max-width: 400px;">
      <pre class="prettyprint lang-python no-margin no-border"><code>def phi(seq):
  seq = np.array(seq)
  return torch.tensor(seq, dtype=torch.float)</code></pre>
    </div>
    <p>
      We generate a trajectory \(\tau\) using the following function. It returns two lists - one that has the rewards \(r_t\) and the other that has the log probabilities of 
      actions, i.e., \(\log(\pi_{\theta}(a_t\vert s_t))\) for all \(t\).
    </p>
    <div class="codeblock" style="max-width: 400px;">
      <pre class="prettyprint lang-python no-margin no-border"><code>def tau():
  d = False
  s = e.reset()  # initial state
  # initial sequence of frames
  seq = collections.deque(
    [np.zeros_like(s)] * (frames - 1) + [s],
    maxlen=frames
  )
  # store rewards and log(pi(a|s))
  Gs, logps = [], []
  while not d:
    s = phi(seq)  # pre-process sequence
    # prob distribution of actions to take
    p = pi(s).squeeze()
    # sample id of action to take
    i = logits(p).sample().item()
    # take action, receive next state and reward
    s, r, d, _ = e.step(actions[i])
    # append new state to sequence
    # so seq has last k frames
    seq.append(s)
    Gs.append(r)  # store reward
    # store log(pi(a|s))
    logps.append(torch.log(p[i]))
  # return rewards and log probabilities
  return Gs, logps</code></pre>
    </div>
    <p>
      We care about the return <b>after</b> timestep \(t\), but <code>Gs[t]</code> contains \(r_t\). We modify this list so <code>Gs[t]</code> is the return
      at timestep \(t\).
    </p>
    <div class="codeblock" style="max-width: 350px;">
      <pre class="prettyprint lang-python no-margin no-border"><code>def accumulate(Gs):
  for t in range(len(Gs) - 2, -1, -1):
    Gs[t] += gamma * Gs[t + 1]
  return np.array(Gs)</code></pre>
    </div>
    <p>
      Next, we calculate the inner sum in \(J\) - \(\sum_{t=0}^{T_k-1}G_t(\tau_k)\log(a_{k, t}\vert s_{k, t})\).
    </p>
    <div class="codeblock" style="max-width: 460px;">
      <pre class="prettyprint lang-python no-margin no-border"><code>def loss(Gs, logps):
  return sum(G * logp for G, logp in zip(Gs, logps))</code></pre>
    </div>
    <p>
      We sometimes normalize the returns array before calculating the above loss.
    </p>
    <div class="codeblock" style="max-width: 450px;">
      <pre class="prettyprint lang-python no-margin no-border"><code>def normalize(x):
  return (x - np.mean(x)) / (np.std(x) + 1e-5)</code></pre>
    </div>
    <p>
      The outer sum - the objective \(J\) - is just the average loss for \(N\) episodes.
    </p>
    <div class="codeblock" style="max-width: 600px;">
      <pre class="prettyprint lang-python no-margin no-border"><code>def meanloss(T):
  return -sum(loss(normalize(accumulate(Gs)), logps) for Gs, logps in T) / len(T)</code></pre>
    </div>
    <p>
      We also want to plot the average loss every epoch, so we define a function for that.
    </p>
    <div class="codeblock" style="max-width: 430px;">
      <pre class="prettyprint lang-python no-margin no-border"><code>def meanGs(T):
  return sum(sum(Gs) for Gs, _ in T) / len(T)</code></pre>
    </div>
    <p>
      Now we define the main training loop which is the PG algorithm.
    </p>
    <div class="codeblock" style="max-width: 450px;">
      <pre class="prettyprint lang-python no-margin no-border"><code>def train():
  returns = []
  for k in range(1, epochs + 1):
    opt.zero_grad()  # reset grad buffer
    # generate N trajectories
    T = tuple(tau() for _ in range(N))
    # store avg return of trajectories
    returns.append(meanGs(T))
    # calculate objective
    J = meanloss(T)
    J.backward()  # backprop
    opt.step()  # update theta
    if k % 5 == 0:
      print(f"episode: {k}\treturn: {returns[-1]}")
  return returns</code></pre>
    </div>
    <p>
      Looks quite similiar to the pseudocode. For CartPole-v0, it works much better than DQL. Trains in 5 seconds for me.
    </p>
    <div align="center">
      <img src="../pg_returns.png" alt="pg" class="height-auto border-05px-grey-solid">
    </div>
  </div>
</body>

</html>