<!DOCTYPE html>
<html>

<head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Backpropagation</title>
    <link rel="stylesheet" href="../style.css">
    <!--katex-->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.10.0-rc.1/dist/katex.min.css"
        integrity="sha384-D+9gmBxUQogRLqvARvNLmA9hS2x//eK1FhVb9PiU86gmcrBrJAQT8okdJ4LMp2uv" crossorigin="anonymous">
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.10.0-rc.1/dist/katex.min.js"
        integrity="sha384-483A6DwYfKeDa0Q52fJmxFXkcPCFfnXMoXblOkJ4JcA8zATN6Tm78UNL72AKk+0O"
        crossorigin="anonymous"></script>
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.10.0-rc.1/dist/contrib/auto-render.min.js"
        integrity="sha384-yACMu8JWxKzSp/C1YV86pzGiQ/l1YUfE8oPuahJQxzehAjEt2GiQuy/BIvl9KyeF" crossorigin="anonymous"
        onload="renderMathInElement(document.body);"></script>
    <!--highlight code-->
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.25.0/themes/prism.min.css"
        integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ=="
        crossorigin="anonymous" referrerpolicy="no-referrer" />
    <script defer src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.25.0/prism.min.js"
        integrity="sha512-hpZ5pDCF2bRCweL5WoA0/N1elet1KYL5mx3LP555Eg/0ZguaHawxNvEjF6O3rufAChs16HVNhEc6blF/rZoowQ=="
        crossorigin="anonymous" referrerpolicy="no-referrer"></script>
    <script defer src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.25.0/plugins/autoloader/prism-autoloader.min.js"
        integrity="sha512-sv0slik/5O0JIPdLBCR2A3XDg/1U3WuDEheZfI/DI5n8Yqc3h5kjrnr46FGBNiUAJF7rE4LHKwQ/SoSLRKAxEA=="
        crossorigin="anonymous" referrerpolicy="no-referrer"></script>
</head>

<body class="margin-0">
    <div class="page">
        <div class="flex align-center justify-center border-top border-bottom">
            <h1>Backpropagation (part one)</h1>
        </div>
        <div class="flex align-center">
            <a class="padding border-bottom" href="../index.html">back</a>
        </div>
        <h2>Regression</h2>
        <p>
            Before we dive into how neural nets (next post!) and backprop work, let's review regression and gradient
            descent.
            Regression is the method by which you find the relationship between one or more independent variable(s)
            \(\small{x}\) and a dependent variable \(\small{y}\). A regression model assumes the following relationship
            holds among the dependent and independent variables
        </p>
        <p class="text-center">
            \(\small{y_i=f(x_i, \theta)+\epsilon}\)
        </p>
        <p>
            where \(\small{\theta}\) is the set of parameters we tune to fit the model, and \(\small{\epsilon}\) is the
            error term (models aren't always perfect!). Let’s take linear regression for example. It’s the simplest
            example of
            regression. You have a bunch of points \(\small{(x_i, y_i)}\), where \(\small{y_i}\) is the dependent
            variable, \(\small{x_i}\) is the independent variable, and we need to find a function
        </p>
        <p class="text-center">
            \(\small{f(x_i, \theta)=\theta_1+\theta_2x_i}\)
        </p>
        <p>
            so \(\small{f(x_i, \theta)}\) is as close as possible to \(\small{y_i}\). More precisely, we need to find
            \(\small{\theta_1}\) (y-intercept) and \(\small{\theta_2}\) (the slope) that best relates all \(\small{(x_i,
            y_i)}\) (where \(\small{\theta=\{\theta_1, \theta_2\}}\) is the set of parameters to tune). How do we find
            optimal \(\small{\theta_1}\) and \(\small{\theta_2}\)? We need a way to measure how wrong the model is.
        </p>
        <p>
            Initially, we choose random values for \(\small{\theta_1}\) and \(\small{\theta_2}\). We tune
            \(\small{\theta_1}\) and \(\small{\theta_2}\) based on how wrong the model is until we reach
            \(\small{\theta^*}\), the optimal value. We measure the model’s wrongness using the mean-square error (MSE)
            loss function. For a data point \(\small{(x_i, y_i)}\), the actual value is \(\small{y_i}\), the model
            prediction is \(\small{f(x_i, \theta)}\), and the MSE for this data point is
        </p>
        <p class="text-center">
            \(\small{\delta_i=(y_i-f(x_i, \theta))^2}\) \(\small{=(y_i-(\theta_1+\theta_2x_i))^2}\)
        </p>
        <p>
            and the total loss is the sum over \(\small{i}\)
        </p>
        <p class="text-center">
            \(\small{L(\theta)=\sum\limits_{i=1}^m\delta_i}\)
            \(\small{=\sum\limits_{i=1}^m(y_i-f(x_i,
            \theta))^2}\) \(\small{=\sum\limits_{i=1}^m(y_i-(\theta_1+\theta_2x_i))^2}\)
        </p>
        <p>
            where \(\small{m}\) is the number of data points. Note that \(\small{L(\theta)}\) is a function of
            \(\small{\theta}\) (here, function of \(\small{\theta_1}\)
            and \(\small{\theta_2}\)), and \(\small{x_i}\) and \(\small{y_i}\) are treated as constants whereas
            \(\small{\theta_1}\) and \(\small{\theta_2}\) are variables. If actual value (or label) \(\small{y_i}\) is
            very different from prediction \(\small{f(x_i, \theta)}\), \(\small{\delta_i}\) is very large. The residual
            (difference b/w label and prediction) is squared because we want to penalise large changes more than
            small changes, and we need an even function because if two terms \(\small{\delta_i}\) and
            \(\small{\delta_j}\) are equal
            in magnitude and opposite in sign, summing them cancels, so loss will be zero, which is not ideal. Squaring
            them means
            \(\small{\delta_i}\) and \(\small{\delta_j}\) will be equal and positive. Suppose \(\small{x_i}\) is 3,
            \(\small{y_i}\) is 5, and \(\small{\theta_1}\) and \(\small{\theta_2}\) are 1 and 6 respectively, the
            prediction is
        </p>
        <p class="text-center">
            \(\small{f(x_i, \theta)=\theta_1+\theta_2x_i}\) \(\small{=1+6(3)=19}\)
        </p>
        <p>
            then the error is
        </p>
        <p class="text-center">
            \(\small{\delta_i=(y_i-f(x_i, \theta))^2}\) \(\small{=(5-19)^2=(-14)^2=196}\)
        </p>
        <p>
            If the residual is doubled, the error quadruples. Consider the above example where \(\small{\theta_1}\) and
            \(\small{\theta_2}\) are 1 and 6 respectively. We have dataset and initial plot below.
        </p>
        <div class="flex justify-center margin-top margin-bottom">
            <table>
                <tr>
                    <td><b>x</b></td>
                    <td>1</td>
                    <td>2</td>
                    <td>4</td>
                    <td>6</td>
                    <td>7</td>
                    <td>11</td>
                    <td>13</td>
                    <td>14</td>
                </tr>
                <tr>
                    <td><b>y</b></td>
                    <td>3</td>
                    <td>5</td>
                    <td>6</td>
                    <td>11</td>
                    <td>12</td>
                    <td>15</td>
                    <td>16</td>
                    <td>21</td>
                </tr>
                <tr>
                    <td><b>\(\small{\delta_i}\)</b></td>
                    <td>16</td>
                    <td>64</td>
                    <td>361</td>
                    <td>676</td>
                    <td>961</td>
                    <td>2704</td>
                    <td>3969</td>
                    <td>4096</td>
                </tr>
            </table>
        </div>
        <div class="flex justify-center">
            <img src="../img/xy.png" alt="plot" class="responsive">
        </div>
        <p>
            When \(\small{\theta_1}\) is 1 and \(\small{\theta_2}\) is 6, the loss (sum \(\small{\delta_i}\)) is 12847.
            Our model is
            way off. Expanding the loss function gives
        </p>
        <p class="text-center overflow-auto">
            \(\small{L(\theta)}\) \(\small{=m\theta_1^2 + \left(\sum\limits_{i=1}^mx_i^2\right)\theta_2^2 -
            \left(2\sum\limits_{i=1}^my_i\right)\theta_1 - \left(2\sum\limits_{i=1}^mx_iy_i\right)\theta_2 +
            \left(2\sum\limits_{i=1}^mx_i\right)\theta_1\theta_2 + \sum\limits_{i=1}^my_i^2}\)
        </p>
        <!--<p class="text-center overflow-auto">
            \(\small{L(\theta)}\) \(\small{=m\theta_1^2 + \left(\sum\limits_{i=1}^mx_i^2\right)\theta_2^2 -
            \left(2\sum\limits_{i=1}^my_i\right)\theta_1 - \left(2\sum\limits_{i=1}^mx_iy_i\right)\theta_2 +
            \left(2\sum\limits_{i=1}^mx_i\right)\theta_1\theta_2 + \sum\limits_{i=1}^my_i^2}\)
        </p>-->
        <p>
            This is of the form
        </p>
        <p class="text-center">
            \(\small{L(\theta)=a\theta_1^2+b\theta_2^2+c\theta_1+d\theta_2+e\theta_1\theta_2+f}\)
        </p>
        <p>
            which is a paraboloid. Substituing values from dataset above, our loss function simplifies to
        </p>
        <p class="text-center">
            \(\small{L(\theta)}\) \(\small{=8\theta_1^2 + 592\theta_2^2 - 178\theta_1 - 1708\theta_2 +
            116\theta_1\theta_2 + 1257}\)
        </p>
        <p>
            The above function gives us loss value for every pair \(\small{(\theta_1, \theta_2)}\), and if you plot it
            (\(\small{\theta_1}\) on x-axis and \(\small{\theta_2}\) on y-axis), it should look something like the
            paraboloid below.
        </p>
        <div class="flex justify-center">
            <img src="../img/x2y2z.png" alt="plot" class="responsive">
        </div>
        <p>
            The loss function here is a paraboloid, and it varies with each dataset since
            the paraboloid
            coefficients are a function of the data points. In the paraboloid, we can see that there is an absolute
            minimum where the gradient
            of \(\small{L}\) w.r.t \(\small{\theta}\) is \(\small{\textbf{0}}\) (I'm assuming knowledge of gradient, and
            maxima/minima of multivariate functions - check out <a
                href="https://www.khanacademy.org/math/multivariable-calculus/multivariable-derivatives"
                class="grey">this</a> and <a
                href="https://www.khanacademy.org/math/multivariable-calculus/applications-of-multivariable-derivatives/optimizing-multivariable-functions-videos/v/multivariable-maxima-and-minima"
                class="grey">this</a>
            for clarification). Here, the loss is lowest, so \(\small{\theta_1}\) and
            \(\small{\theta_2}\) here is optimal. We can find derivatives w.r.t to \(\small{\theta_1}\)
            and \(\small{\theta_2}\) and solve the system of equations to find the optimal values.
        </p>
        <p class="text-center">
            \(\small{\nabla L=\begin{bmatrix}\frac{\partial L}{\partial \theta_1} \\ \\ \frac{\partial
            L}{\partial
            \theta_2}\end{bmatrix}}\) \(\small{=\textbf{0}}\)
        </p>
        <p>
            expanding above
        </p>
        <p class="text-center">
            \(\small{\nabla L=\begin{bmatrix}2\sum\limits_{i=1}^m(\theta_1+\theta_2x_i-y_i) \\ \\
            2\sum\limits_{i=1}^m(\theta_1+\theta_2x_i-y_i)x_i\end{bmatrix}}\)
            \(\small{=\begin{bmatrix}16\theta_1 + 116\theta_2 -178\\ \\ 1184\theta_2 + 116\theta_1 -
            1708\end{bmatrix}=\begin{bmatrix}0 \\ \\ 0\end{bmatrix}}\)
        </p>
        <p>
            and rearranging gives
        </p>
        <p class="text-center">
            \(\small{\begin{bmatrix}16\theta_1 + 116\theta_2\\ \\ 116\theta_1\ + 1184\theta_2
            \end{bmatrix}=\begin{bmatrix}178 \\ \\ 1708\end{bmatrix}}\)
        </p>
        <p>
            which is a simple system of linear equations. Solving them gives \(\small{\theta_1=2.30029}\) and
            \(\small{\theta_2= 1.2172}\).
            So the line of best fit is \(\small{f(x_i, \theta)=2.30029+1.2172x_i}\) because the absolute minimum of
            \(\small{L}\) is at \(\small{(2.30029, 1.2172)}\).
        </p>
        <div class="flex justify-center">
            <img src="../img/best_fit.png" alt="best_fit" class="responsive">
        </div>
        <p>
            For simple linear regression, the general solution is
        </p>
        <p class="text-center overflow-auto">
            <span>\(\small{\theta_1=\dfrac{(\sum\limits_{i=1}^my_i)(\sum\limits_{i=1}^mx_i^2)-(\sum\limits_{i=1}^mx_i)(\sum\limits_{i=1}^mx_iy_i)}{m(\sum\limits_{i=1}^mx_i^2)-(\sum\limits_{i=1}^mx_i)^2}}\)</span>
            <br>
            <span>\(\small{\theta_2=\dfrac{m(\sum\limits_{i=1}^mx_iy_i)-(\sum\limits_{i=1}^mx_i)(\sum\limits_{i=1}^my_i)}{m(\sum\limits_{i=1}^mx_i^2)-(\sum\limits_{i=1}^mx_i)^2}}\)</span>
        </p>
        <p>
            Looks scary, but it's just basic symbol manipulation. Try coding it up in Python or something - and try
            deriving it yourself! Look at the system of equations formed by the relation \(\small{\nabla L=0}\) and try
            solving for \(\small{\theta_1}\) and \(\small{\theta_2}\). Above (before the plot), we have substituted, all
            the \(\small{x_i}\) and \(\small{y_i]\) values and arrived at the system of equations for *this* data set.
            Try solving it for the general case.
        </p>
        </p>
        Anyways, we have arrived
        at an explicit solution to the optimisation problem of finding the straight line that best fits a bunch of
        points. Here,
        we only needed to find two parameters. What if you want to model more complex relationships? What if you
        want to use more
        than two parameters? Ten parameters? Ten thousand? Ten billion? We cannot find an explicit solution when the
        problem becomes
        more complex. We can, however, find an approximate solution by using a technique called gradient descent.
        </p>
        <br>
        <h2>Gradient Descent</h2>
        <p>
            The gradient of a multivariate function at a point is a vector that points in the direction of steepest
            ascent. For a
            function \(\small{z=f(\textbf{x})}\) (where \(\small{\textbf{x}=(x_1, \dots, x_n)}\) is a point in
            \(\small{n}\) dimensions),
            the gradient at a point \(\small{p}\) is defined as
        </p>
        <p class="text-center">
            \(\small{\nabla f|_p=\begin{bmatrix}\frac{\partial f}{\partial x_1}|_p\\ \vdots \\ \frac{\partial
            f}{\partial
            x_n}|_p\end{bmatrix}}\)
        </p>
        <p>
            where \(\small{|_p}\) denotes "evaluated at \(\small{p}\)". If \(\small{f(\textbf{x})=x_1^2+x_2^2}\), then
            \(\small{\frac{\partial f}{\partial x_1}|_2=(2x_1)|_2=2(2)=4}\). Here, \(\small{f}\) is a function of two
            variables. Suppose
            \(\small{p=(x_1, x_2)=(1, 2)}\). \(\small{f}\) is \(\small{1^2+2^2=5}\). Now suppose from \(\small{p}\) you
            step in some
            direction \(\small{dp=(0.01, 0.2)}\). The new point is \(\small{p + dp}\). How much will \(\small{f}\) have
            changed?
            Adding \(\small{\partial x_1}\) to \(\small{x_1}\) means \(\small{f}\) will increase by
            \(\small{\frac{\partial f}{\partial x_1}|_p}\partial x_1\) (this is just basic calculus, change in function
            equals derivative times change in x),
            and adding \(\small{\partial x_2}\) to \(\small{x_2}\) means \(\small{f}\) will increase by
            \(\small{\frac{\partial f}{\partial x_2}|_p}\partial x_2\),
            so the total change is
        </p>
        <p class="text-center">
            \(\small{f(p + dp)-f(p)}\) \(\small{=\frac{\partial f}{\partial x_1}|_p}\partial x_1 + \frac{\partial
            f}{\partial x_2|_p}\partial x_2\)
            \(\small{=\nabla f|_p\cdot dp}\)
        </p>
        <p>
            So value of \(\small{f}\) changes by \(\small{\nabla f|_p\cdot dp}\) if we go from \(\small{p}\) to
            \(\small{p + dp}\) where
            \(\small{dp}\) is some infinitesimal vector. From this, we can see why the gradient is the direction of
            steepest ascent.
        </p>
        <p class="text-center">
            \(\small{\nabla f|_p\cdot dp}\) \(\small{=||\nabla f|_p||||dp||\cos\theta}\)
        </p>
        <p>
            Above expression is maximum when \(\small{\cos\theta}\) is maximum which is when \(\small{\theta}\) is 0,
            i.e., when \(\small{dp}\)
            is along \(\small{\nabla f|_p}\), which is why the gradient is along the direction of steepest ascent. It is
            defined that way.
            We can now see how gradient descent works.
        </p>
        <p>
            We initially start at some random point, \(\small{\theta}\) in the case of some loss function.
            We compute the gradient at that point. We scale it by some small constant (called learning rate)
            \(\small{\eta}\), and step
            in the direction opposite the gradient (because gradient is direction of steepest ascent, minus gradient is
            direction of steepest descent). We keep stepping in the direction of steepest descent, until we hit a local
            minima (a bowl, if you may).
            Here, the gradient is 0 (or just really, really small, because we don't usually hit zero gradient in
            practice).
        </p>
        <div class="flex justify-center">
            <div style="width: clamp(256px, 70%, 400px);" class="overflow-scroll">
                <p class="border-top border-bottom"><b>Algorithm</b> Gradient Descent</p>
                <p>
                    Initialise \(\small{\theta}\) randomly
                    <br>
                    Until \(\small{L\approx 0}\)
                    <br>
                    <span class="padding-left">\(\small{\theta\coloneqq\theta-\eta\nabla L|_{\theta}}\)</span>
                </p>
            </div>
        </div>
        <p>
            where \(\small{L}\) is some objective function.
        </p>
        <div class="flex justify-center">
            <img src="../img/to_minima.png" alt="minima" class="responsive">
            <img src="../img/grad_descent.gif" alt="gradient_descent" classs="responsive">
            <span class="font-08em grey">There may be multiple local minima, and different initial conditions lead to
                different minima - in our case, paraboloid as only one minima which is the absolute minimum.</span>
        </div>
        <p>
            Let's use gradient descent to solve linear regression. The derivatives of the loss function we first
            discussed are
        </p>
        <p class="text-center">
            \(\small{\dfrac{\partial L}{\partial \theta_1}}\)
            \(\small{=2\sum\limits_{i=1}^m​(\theta_1​+\theta_2​x_i​−y_i​)=16\theta_1​+116\theta_2​−178}\)
            <br>
            <br>
            \(\small{\dfrac{\partial L}{\partial \theta_2}}\)
            \(\small{=2\sum\limits_{i=1}^m​(\theta_1​+\theta_2​x_i​−y_i​)x_i=116\theta_1​+1184\theta_2​−1708}\)
        </p>
        <p>
            and so the gradient is
        </p>
        <p class="text-center">
            \(\small{\nabla L=\begin{bmatrix}16\theta_1+116\theta_2-178 \\ \\
            116\theta_1+1184\theta_2-1708\end{bmatrix}}\)
        </p>
        <p>
            Remember that \(\small{\theta_1}\) and \(\small{\theta_2}\) are 1 and 6 initially. Substituing in above
        </p>
        <p class="text-center">
            \(\small{\nabla L=\begin{bmatrix}534 \\ \\ 5512\end{bmatrix}}\)
        </p>
        <p>
            i.e., \(\small{\frac{\partial L}{\partial \theta_1}|_1=534}\), and \(\small{\frac{\partial L}{\partial
            \theta_2}|_6=5512}\).
            Multiply both of them by learning rate \(\small{\eta=0.001}\) and we have
        </p>
        <p class="text-center">
            \(\small{\partial \theta=\eta\nabla L=\begin{bmatrix}0.534 \\ \\ 5.512\end{bmatrix}}\)
        </p>
        <p>
            Now we update \(\small{\theta}\), i.e., subtract 0.534 from \(\small{\theta_1}\) to get new
            \(\small{\theta_1}\) value
            0.466, and subtract 5.512 from \(\small{\theta_2}\) to get new \(\small{\theta_2}\) value 0.488. Now we do
            the same for the
            new values. Subsitute them into \(\small{\nabla L}\), multiply by \(\small{\eta}\), and then subtract. We do
            this over and over
            until \(\small{\theta}\) gets sufficiently close to optimal value. For \(\small{\theta=0.001}\), 2000 steps
            gives us solution close
            to four decimal places.
        </p>
        <div class="flex justify-center overflow-auto">
            <img src="../img/df.png" alt="df" style="max-width: 200px;">
        </div>
        <p>
            In Python,
        </p>
        <div class="flex justify-center">
            <pre class="background-white font-08em border overflow-auto padding"><code class="language-python"># for dataset above, following are loss function derivatives 

# derivative of L w.r.t Θ1
def dLdΘ1(Θ1, Θ2):
    return 16 * Θ1 + 116 * Θ2 - 178


# derivative of L w.r.t Θ2
def dLdΘ2(Θ1, Θ2):
    return 116 * Θ1 + 1184 * Θ2 - 1708


Θ1, Θ2 = 1, 6  # initial values
η = 0.001  # learning rate
for i in range(2000):
    dΘ1 = η * dLdΘ1(Θ1, Θ2)
    dΘ2 = η * dLdΘ2(Θ1, Θ2)
    Θ1 = Θ1 - dΘ1
    Θ2 = Θ2 - dΘ2

print(Θ1, Θ2)
</code></pre>
        </div>
        <p>
            In this way, we calculate the gradient of the loss function (which will have thousands to billions of
            parameters if we're using a neural net) by computing the derivative of the loss function w.r.t every single
            parameter at a point (here, there are only two parameters), multiplying the derivatives by a small constant
            \(\small{\eta}\), and subtracting it from the parameters, and repeating the process over and over again
            until optimality.
        </p>
        <p>
            In the next post, we'll see how one can calculate
            gradients of super complex
            functions like neural nets using matrix operations, and then in the same post a bit about automatic
            differentiation as an alternative
            to matrix operations, and in the final one (if I get to it), implementing automatic differentiation in
            Python (similar to PyTorch Autograd).
        </p>
    </div>
</body>

</html>