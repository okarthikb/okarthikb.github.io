---
title: "Just a Token Predictor"
date: 2024-01-01
---

### Compression and intelligence

A popular argument against the "They're just next token predictors, so they don't understand anything!" critique of language models is that text prediction is compression, and compression leads to understanding. We turn to Ilya Sutskever's rather prescient quote from a [2011 paper](https://icml.cc/2011/papers/524_icmlpaper.pdf) on RNNs:

*More speculatively, achieving the asymptotic limit in text compression requires an understanding that is "equivalent to intelligence" (Hutter, 2006).*
