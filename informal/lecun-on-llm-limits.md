---
title: "Yann LeCun on LLM Limitations"
date: 2024-01-01
---

### Yann Lecun tweet explained (?)

<blockquote class="twitter-tweet">
<p lang="en" dir="ltr">* Language is low bandwidth: less than 12 bytes/second. A person can read 270 words/minutes, or 4.5 words/second, which is 12 bytes/s (assuming 2 bytes per token and 0.75 words per token). A modern LLM is typically trained with 1x10^13 two-byte tokens, which is 2x10^13 bytes.â€¦ <a href="https://t.co/FtCnxkVukK">https://t.co/FtCnxkVukK</a></p>&mdash; Yann LeCun (@ylecun) <a href="https://twitter.com/ylecun/status/1766498677751787723?ref_src=twsrc%5Etfw">March 9, 2024</a>
</blockquote>

Now, Yann has thrown a lot of words here, but let's view it from the lens of [Moravec's paradox](https://en.wikipedia.org/wiki/Moravec's_paradox) because it's largely similar (in my opinion).

What does Yann mean by "...redundancy in data is *precisely* what we need for Self-Supervised Learning to capture the structure of the data"? So far, I have tried coming up with two similar explanations for two people who asked few days ago so here's the gist.

(Below is messages formatted by Claude, removing some errors, so might have some issues)

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">alright, *specifically* what intellectual tasks can a 4-year-old do that GPT-4 cannot? <a href="https://t.co/OwJU5wv2W5">https://t.co/OwJU5wv2W5</a></p>&mdash; Lewis (@ctjlewis) <a href="https://twitter.com/ctjlewis/status/1765617960373412261?ref_src=twsrc%5Etfw">March 7, 2024</a></blockquote>

A 4-year-old can play 20 questions spontaneously. They are "embedded agents" - intuitively understanding the relation between them, their thoughts, and the world. They can answer "where and when did you learn this?" - the questions that are of interest for autonomous, agentic systems aren't the questions GPT-4 can easily answer. Answering graduate-level math questions is in no way going to help you with, say, something that can help you run a factory or engage with you daily in the physical world (where you don't have a laptop in front of you) over long time horizons. 4-year-olds have an intuitive physics model; a lot of knowledge is tacit and can't be said in words.

Any question to do with episodic memory - you could say that by increasing the context length, ChatGPT can remember old conversations. But I'm not sure you can fit an entire life in ChatGPT's context length because of a lack of grounded reasoning: tacit knowledge of the current state of the world. Note that ChatGPT can only tell you the current date if it's in the system prompt. The date is a real fact about the current state of the world and is just a few bytes long snippet of information. The set of all information about the current state of the world, including those very contextually specific about the agent, however, can't be specified using just a few bytes of words (such as temperature, mood, weather, human interactions, the things you see as you interact - basically short term, episodic, and long term history of data ingested by your senses and internal climate like hormones and thoughts conditioning your current behavior) -- you can't fit this in a simple "system prompt" because you cannot state them in words. Think of how incredibly sensitive your fingers are, the resolution of nerve input. When we say "ChatGPT is not grounded," it doesn't mean it's stupid, it means it doesn't have access to all the information required to answer a particular question, because getting that information is an engineering problem that can only be solved by building robots that get real-time information from the world across modalities. This is what Moravec's paradox is about. Text is way too compressed to get all that information, and so any question in this category is not answerable by an LLM.

The difference between AGI (things that replace human labor) and GPT-4 (something that can answer complex questions) is tacit knowledge about the world. A simple example: interacting with ChatGPT voice. When you ask it to "read the code aloud," it will keep telling you to open the app and look at the markdown because that's how they programmed it - "if markdown generated, ask user to open the app and read the code." It doesn't understand the interface with which it interacts with the world. You need to deliberately talk to it and explain, "When you generate markdown, the TTS logic tells the user to look at the app. Instead, please don't write the code in markdown but read it word by word by generating a paragraph with all the words separated by a space. Replace the symbols with their name in the para, 'bracket', 'comma', and so on." These are the kinds of questions it can't answer. A 4-year-old intuitively understands how they're interfaced with the world. You don't notice that they're able to do this because it's so embedded in the way animals and humans behave - we find it natural. ChatGPT doesn't. This is why if tomorrow it can suddenly get a gold in all the Olympiads, the world won't change at all, because you still haven't solved the engineering problem of interfacing the system with real-time information about the world so it can intuitively model and interact with it. In short, instead of measuring AGI by asking "what sort of questions it can answer," we should measure by "what can it do." Measure by ability to do labor, not exam scores and academic benchmarks.

Coming to the Figure demo: the demo does ground the LLM in the real world by interfacing it properly, but like the ChatGPT TTS example above, you'll be able to find very blatant issues of the same sort after interacting with it for some time. To get at where it might fail, you don't think of simply hard reasoning problems but about how the system design is set up - how are the various models integrated with one another and where does information about grounding fail to pass? Finding these failure mode questions will get harder and harder as more people do the sort of stuff like Figure does, but in my limited experience I'd still say that a lot of human labor requires bypassing these failure modes *very* consistently over a long time horizon.

The issue is also how people interpret Yann. When Yann says "intelligent," he doesn't mean "something that can answer a large range of questions" - he also includes grounding, self-awareness, etc., in his definition. The latter is actually what matters for generality, because when you talk about "ability to do labor," you include the whole range of human activity, whereas if by intelligence you mean ability to answer questions, then you include only, say, white-collar work like math research or code assistance. If you only give a minute, a young teen may lose to an LLM in generating some bit of code. However, if you consider a span of a month or two, you can expect a teen to self-learn and make a reasonable piece of software, whereas without manually teaching the model to plan and an agent API, you can't expect it to automatically do anything. Even if ChatGPT has more knowledge, you're still going to trust a teenager with "Hey, I'll give you a month, make this software for me." LLM interactions last at most a few minutes, so Yann seems perfectly in the right to me. The slow nature of human reasoning is a pro not really a con.

To answer the question about redundancy being good for SSL: redundancy here refers to learning the structure of data, as Yann mentions. So, for example, take the task "riding a bicycle." This phrase compresses an enormous amount of occurrences of riding a bicycle in the world. You can point at so many things and correctly call it riding a bicycle. This enables abstract reasoning for LLMs, by compression. Right after GPT-3 came out and a lot of people were underestimating how far GPTs can go, a few people without academia brain spotted why it's being underestimated: people didn't read what eminent AGI researchers (Ilya, Shane Legg, Hutter, ...) had to say about text compression:

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">there are several reasons why it&#39;s the best way:<br><br>1. information dense. text leaves out uncorrelated information about reality in general. images will capture every pixel possible in frame and treat each the same.</p>&mdash; alth0u&#x1f938; (@alth0u) <a href="https://twitter.com/alth0u/status/1295140716771831810?ref_src=twsrc%5Etfw">August 16, 2020</a></blockquote>

But compression (and some RLHF to imbue turn-based agency on top) can only take you so far in the physical world.

When it talks about riding a bicycle, it abstractly does understand it. But to actually ride a bicycle, you need to process all the context-specific data about riding a bicycle. That means all the sensory data the person processes while riding a bicycle, which is going to be in the gigabytes or terabytes if you consider all the data (touch, vision, smell, sound, instructions from people, observations of others cycling).

This goes exactly against the point @alth0u is making in the tweet above about how "text leaves uncorrelated information about reality" - in short it leaves only the non-redundant aspects of reality.

And because you remove all this redundant information (this is what he means by redundant - the data I mentioned is redundant to grok the abstract concept of riding a bicycle, but not redundant to learn to actually ride one), the model can only answer physics intuition puzzles in words, which gives (more often than not, but not always) the illusion that it can now do counterfactual reasoning about real physics (which is what "having a world model" actually means in a technical sense). We also take for granted our ability to have a sense of time. In general, we take for granted the vast amount of sensory input we're processing to inform the language or code we generate and think language is the hard part (as Moravec's paradox says), but it's hard for us *because* it's at the end of doing vast amounts of SSL.

The riding a bicycle example is easy because it's in the line of "but can it clean my room?" But the same reason why riding a bicycle is hard for an LM is the same reason why they score high on Olympiad or graduate textbook questions but score a paltry 4% on SWE bench. Because software engineering is not chatting or answering questions in a bounded environment. SWE is human labor that requires processing a lot of "redundant" data the same way riding a bicycle does, and this data gets lost when you try to compress everything into language. LLMs are a great proof for "answering questions about thing doesn't mean you understand thing."

Regarding the lack of sense of time: how would you give it a sense of time, say, for running programs? To stop in case one takes too long? Simple: you'd ask to import time in the code it writes and print out the time it takes every time the program runs, or you augment the inference loop so you're passing in the delta of time from the start of the conversation or message, and the model can send a "terminate" signal using a function call to stop the program and continue the chat loop to edit the program. But look at what you just did! You grounded the language model by passing real information about the state of the world, in this case, time. But passing time information is just a few bytes because it's only a few tokens ("<number> seconds passed" isn't that many bytes). But, as I said before, and as Yann said, to do real-world tasks and ground the language model, you need to pass in all the sensory data, which is gonna be a hell of a lot more bytes. *You're bottlenecked by perception data processing to be used for grounding the agent in the real world.*

![Perception bottleneck](/images/perception-bottleneck.jpg)

This is precisely - as Yann explains - what's happening in this Devin demo where they ask to control a robot.

And this is also largely what Figure seemed to solve -- processing lots of sensory load, and only using the LLM to solve super high level plans. The language modeling is a completely separate system here and is the orchestrator, but for humans, language generation is more like tool use - it is conditioned on high level plans made in latent representation of the environment, rather than language being the main controller.

A tl;dr diagram to go: (I'll add it tomorrow, not patient enough for excalidraw today)
