<!DOCTYPE html>
<html>

<head>
  <title>Vanilla policy gradient, DQL, and A2C</title>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <link rel="stylesheet" href="style.css">

  <!-- syntax highlighting -->
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.4.0/styles/default.min.css">
  <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.5.0/highlight.min.js"></script>
  <script>hljs.initHighlightingOnLoad();</script>

  <!-- MathJax -->
  <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      "HTML-CSS": { 
        preferredFont: "TeX", 
        availableFonts: ["STIX", "TeX"], 
        linebreaks: { automatic: true }, 
        EqnChunk: (MathJax.Hub.Browser.isMobile ? 10 : 50) 
      },
      tex2jax: { 
        inlineMath: [ ["\\(", "\\)"], ["\$", "\$"] ], 
        displayMath: [ ["$$","$$"], ["\\[", "\\]"] ], 
        processEscapes: true, 
        ignoreClass: "tex2jax_ignore|dno",
        skipTags: ["script", "noscript", "style", "textarea"]
      },
      TeX: {  noUndefined: { attributes: { mathcolor: "red", mathbackground: "#FFEEEE", mathsize: "90%" } } },
      messageStyle: "none"
    });
  </script>
  <script type="text/javascript" async
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-MML-AM_CHTML"></script>

  <!-- js -->
  <script async src="script.js"></script>
</head>

<body>
  <div class="page">
    <h2>Vanilla policy gradient, DQL, and A2C</h2>

    <p><a href="index.html" style="font-size: 1.2em;">‚Üê</a></p>

    <br>

    <h3>Intro</h3>

    <p>
      In this post, I'll explain how to implement VPG, DQL, and A2C in Python using PyTorch and OpenAI's gym. I'm assuming
      familiarity with basic RL concepts (watch <u><a href="https://www.youtube.com/watch?v=TCCjZe0y4Qc">this</a></u> 
      video for a revision). Take a look at gym's <u><a href="https://gym.openai.com/docs/">documentation</a></u> as well.
    </p>

    <br>

    <h3>Vanilla policy gradient</h3>

    <p>
      The idea behind policy gradient is to increase (or decrease) the probability of actions taken in proportion to returns.
      $\pi_\theta$ is the policy model parametrized by $\theta$, and $\pi_\theta(s)$ is the probability distribution of actions 
      given the environment state $s$. We need to find $\theta^*$ that maximizes the expected reward objective function
    </p>

    <div class="ovf">
      $$J(\theta) = \mathbb{E}_{a \sim \pi_\theta}\Big[G = \sum_{t=0}^T \gamma^t r_t\Big] = \int_\tau \mathcal{R}(\tau)p_\theta(\tau). \tag{1}$$
    </div>

    <p>
      $\mathcal{R}(\tau)$ is the return for trajectory $\tau$ (a trajectory or episode is a sequence of state-action-reward
      tuples $\{(s_0, a_0, r_0),\ldots,(s_T, a_T, r_T)\}$) and $p_\theta(\tau)$ is the probability the agent follows trajectory
      $\tau$. We use gradient ascent to maximize the objective function.
    </p>

    <div class="ovf">
      $$\theta \leftarrow \theta + \eta\nabla_\theta J(\theta) \tag{2}$$
    </div>

    <p>
      where $\eta$ is the learning rate. But how do we compute $\nabla_\theta J(\theta)$? We start with the integral in $(1)$. 
      We use what's called the log trick to simplify
    </p>

    <div class="ovf">
      $$\nabla_\theta f(\theta) = f(\theta)\nabla_\theta\log(f(\theta)).$$
    </div>

    <p>
      Taking gradient on both sides of $(1)$
    </p>

    <div class="ovf">
      \begin{eqnarray}
        \nabla_\theta J(\theta) &=& \nabla_\theta \int_{\tau}\mathcal{R}(\tau)p_\theta(\tau) \\
        &=& \int_{\tau}\mathcal{R}(\tau)\nabla_\theta p_\theta(\tau) \\
        &=& \int_{\tau}\mathcal{R}(\tau)\nabla_\theta\log (p_\theta(\tau)) p_\theta(\tau) \tag{log trick} \\
        &=& \mathbb{E}\Big[\mathcal{R}(\tau)\nabla_\theta\log (p_\theta(\tau))\Big] \\
        &\approx& \frac{1}{N}\sum_{i=0}^N \mathcal{R}(\tau_i)\nabla_\theta\log(p_\theta(\tau_i)). \tag{3}
      \end{eqnarray}
    </div>

    <p>
      We simplify further. Consider $p_\theta(\tau_i)$ in $(3)$. It's the probability the agent will follow trajectory
      $\tau_i$, and this probability is dependent on $\theta$. By definition, $\pi_\theta(a \vert s)$ is the probability the 
      agent will take action $a$ in state $s$. Let $P(s' \vert s, a)$ be the probability the environment will transition to state 
      $s'$ given that the agent took action $a$ in state $s$. Then the transition probability of the agent taking action $a$ in 
      state $s$ and moving to state $s'$ is given by $P(s' \vert s, a)\pi_\theta(a \vert s)$. The probability of trajectory 
      $\tau_i$ is the product of all transition probabilities
    </p>

    <div class="ovf">
      $$p_\theta(\tau_i) = \prod_{t=0}^{n(\tau_i)} P(s_{i, t + 1} \vert s_{i, t}, a_{i, t})\pi_\theta(a_{i, t} \vert s_{i, t})$$
    </div>

    <p>
      where $\tau_i$ refers to the $i$th trajectory, and $s_{i, t}$ refers to the state at timestep $t$ of trajectory $i$.
      Substitute the above in $\nabla_\theta\log(p_\theta(\tau_i))$ in $(3)$
    </p>

    
  </div>
</body>

</html>