<!DOCTYPE html>
<html>

<head>
  <title>Vanilla policy gradient, DQL, and A2C</title>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <link rel="stylesheet" href="style.css">

  <!-- syntax highlighting -->
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.4.0/styles/default.min.css">
  <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.5.0/highlight.min.js"></script>
  <script>hljs.initHighlightingOnLoad();</script>

  <!-- MathJax -->
  <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      "HTML-CSS": { 
        preferredFont: "TeX", 
        availableFonts: ["STIX", "TeX"], 
        linebreaks: { automatic: true }, 
        EqnChunk: (MathJax.Hub.Browser.isMobile ? 10 : 50) 
      },
      tex2jax: { 
        inlineMath: [ ["\\(", "\\)"], ["\$", "\$"] ], 
        displayMath: [ ["$$","$$"], ["\\[", "\\]"] ], 
        processEscapes: true, 
        ignoreClass: "tex2jax_ignore|dno",
        skipTags: ["script", "noscript", "style", "textarea"]
      },
      TeX: {  noUndefined: { attributes: { mathcolor: "red", mathbackground: "#FFEEEE", mathsize: "90%" } } },
      messageStyle: "none"
    });
  </script>
  <script type="text/javascript" async
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-MML-AM_CHTML"></script>

  <!-- js -->
  <script async src="script.js"></script>
</head>

<body>
  <div class="page">
    <h2>Vanilla policy gradient, DQL, and A2C</h2>

    <p><a href="index.html" style="font-size: 1.2em;">‚Üê</a></p>

    <br>

    <h3>Intro</h3>

    <p>
      In this post, I'll explain how to implement VPG, DQL, and A2C in Python using PyTorch and OpenAI's gym. I'm assuming
      familiarity with basic RL concepts (watch <u><a href="https://www.youtube.com/watch?v=TCCjZe0y4Qc">this</a></u> 
      video for a revision). Take a look at gym's <u><a href="https://gym.openai.com/docs/">documentation</a></u> as well.
    </p>

    <br>

    <h3>Vanilla policy gradient</h3>

    <p>
      The idea behind policy gradient is to increase (or decrease) the probability of actions taken in proportion to returns.
      $\pi_\theta$ is the policy model parametrized by $\theta$, and $\pi_\theta(s)$ is the probability distribution of actions 
      given the environment state $s$. We need to find $\theta^*$ that maximizes the expected reward objective function
    </p>

    <div class="ovf">
      $$J(\theta) = \mathbb{E}_{a \sim \pi_\theta}\Big[G = \sum_{t=0}^T \gamma^t r_t\Big] = \int_\tau \mathcal{R}(\tau)p_\theta(\tau). \tag{1}$$
    </div>

    <p>
      $\mathcal{R}(\tau)$ is the return for trajectory $\tau$ (a trajectory or episode is a sequence of state-action-reward
      tuples $\{(s_0, a_0, r_0),\ldots,(s_T, a_T, r_T)\}$) and $p_\theta(\tau)$ is the probability the agent follows trajectory
      $\tau$. We use gradient ascent to maximize the objective function.
    </p>

    <div class="ovf">
      $$\theta \leftarrow \theta + \eta\nabla_\theta J(\theta)$$
    </div>

    <p>
      where $\eta$ is the learning rate. But how do we compute $\nabla_\theta J(\theta)$? We start with the integral in $(1)$.
    </p>
  </div>
</body>

</html>